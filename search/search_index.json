{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Get started","text":"<p> Declarai, turning Python code into LLM tasks, easy to use, and production-ready. </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Declarai turns your Python code into LLM tasks, utilizing python's native syntax, like type hints and docstrings, to instruct an AI model on what to do.</p> <p>Designed with a clear focus on developer experience, you simply write Python code as you normally would, and Declarai handles the rest.</p> <p>poem_generator.py<pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.task\ndef generate_poem(title: str) -&gt; str:\n\"\"\"\n    Write a 4 line poem on the provided title\n    \"\"\"\n\n\nres = generate_poem(\n    title=\"Declarai, the declarative framework for LLMs\"\n)\nprint(res)\n</code></pre> <pre><code>&gt;&gt;&gt; Declarai, the AI framework,\n... Empowers LLMs with declarative power,\n... Efficiently transforming data and knowledge,\n... Unlocking insights in every hour.\n</code></pre></p>"},{"location":"#why-use-declarai","title":"Why use Declarai?","text":"<ul> <li> <p>Pythonic Interface to LLM: - Leverage your existing Python skills instead of spending unnecessary time creating complex prompts.</p> </li> <li> <p>Lightweight: - Declarai is written almost solely in python 3.6 using only pydantic and openai SDKs, so there's no need to worry about dependency spaghetti.</p> </li> <li> <p>Extendable: - Declarai is designed to be easily extendable, the interface is simple and accessible by design so   you can easily override or customize the behavior of the framework to your specific needs.</p> </li> <li> <p>Type-Driven Prompt Design: - Through the application of Python's type annotations,    Declarai constructs detailed prompts that guide Large Language Models (LLMs) to generate the desired output type.</p> </li> <li> <p>Context-Informed Prompts via Docstrings: - Implement function docstrings to supply contextual data to LLMs,    augmenting their comprehension of the designated task, thereby boosting their performance.</p> </li> <li> <p>Automated Execution of LLM Tasks: - Declarai takes care of the execution code, letting you concentrate on the core business logic.</p> </li> </ul> <p>Utilizing Declarai leads to improved code readability, maintainability, and predictability.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install declarai\n\n---&gt; 100%\nDone!\n</code></pre>"},{"location":"#feature-highlight","title":"Feature highlight","text":""},{"location":"#python-native-syntax","title":"Python native syntax","text":"<p>Integrating deeply into python's native syntax, declarai understands your code and generates the prompt accordingly.</p> Simple Syntax<pre><code>@declarai.task # (1)!\ndef rank_by_severity(message: str) -&gt; int: # (2)!\n\"\"\"\n    Rank the severity of the provided message by it's urgency.\n    Urgency is ranked on a scale of 1-5, with 5 being the most urgent.\n    :param message: The message to rank\n    :return: The urgency of the message\n    \"\"\" # (3)!\n\n\nprint(rank_by_severity(message=\"The server is down!\"))\n#&gt; 5\nprint(rank_by_severity(message=\"How was your weekend?\"))\n#&gt; 1\n</code></pre> <ol> <li>The <code>@declarai.task</code> decorator marks the function as a Declarai prompt task.</li> <li>The type hints <code>List[str]</code> are used to parse the output of the llm into a list of strings.</li> <li>The docstring represents the task's description which is used to generate the prompt.<ul> <li><code>description</code> - the context of the task</li> <li><code>:param</code> - The function's parameters and their description</li> <li><code>:return</code> - The output description</li> </ul> </li> </ol>"},{"location":"#support-python-typing-and-pydantic-models","title":"Support Python typing and pydantic models","text":"<p>Declarai will return a serialized object as defined by the type hints at runtime. Builtins<pre><code>@declarai.task\ndef extract_phone_number(email_content: str) -&gt; List[str]:\n\"\"\"\n    Extract the phone numbers from the provided email_content\n    :param email_content: Text that represents the email content \n    :return: The phone numbers that are used in the email\n    \"\"\"\n\nprint(extract_phone_number(email_content=\"Hi, my phone number is 123-456-7890\"))\n#&gt; ['123-456-7890']\n</code></pre></p> Builtins<pre><code>@declarai.task\ndef datetime_parser(raw_date: str) -&gt; datetime:\n\"\"\"\n    Parse the input into a valid datetime string of the format YYYY-mm-ddThh:mm:ss\n    :param raw_date: The provided raw date\n    :return: The parsed datetime output\n    \"\"\"\n\n\nprint(datetime_parser(raw_date=\"Janury 1st 2020\"))\n#&gt; 2020-01-01 00:00:00\n</code></pre> <p>Pydantic models<pre><code>class Animal(BaseModel):\n    name: str\n    family: str\n    leg_count: int\n\n\n@declarai.task\ndef suggest_animals(location: str) -&gt; Dict[int, List[Animal]]:\n\"\"\"\n    Create a list of numbers from 0 to 5\n    for each number, suggest a list of animals with that number of legs\n    :param location: The location where the animals can be found\n    :return: A list of animal leg count and for each count, the corresponding animals\n    \"\"\"\n\n\nprint(suggest_animals(location=\"jungle\"))\n#&gt; {\n#       0: [\n#           Animal(name='snake', family='reptile', leg_count=0)\n#       ], \n#       2: [\n#           Animal(name='monkey', family='mammal', leg_count=2), \n#           Animal(name='parrot', family='bird', leg_count=2)\n#       ], \n#       4: [\n#          Animal(name='tiger', family='mammal', leg_count=4), \n#          Animal(name='elephant', family='mammal', leg_count=4)\n#       ]\n# }\n</code></pre> </p>"},{"location":"#chat-interface","title":"Chat interface","text":"<p>Create chat interfaces with ease, simply by writing a class with docstrings</p> <p>Info</p> <p>Notice that <code>chat</code> is exposed under the <code>experimental</code> namespace, noting this interface is still work in progress.</p> <p><pre><code>@declarai.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    \"\"\"\n\n    def send(self, message: str) -&gt; int: ...\n\n\ncalc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 1\"))\n#&gt; 2\n</code></pre> </p>"},{"location":"#task-middlewares","title":"Task Middlewares","text":"<p>Easy to use middlewares provided out of the box as well as the ability to easily create your own.</p> <p>Logging Middleware<pre><code>@declarai.task(middlewares=[LoggingMiddleware])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return Declarai.magic(text=text)\n\nres = extract_info(\n    text=\"Hey jenny,\"\n    \"you can call me at 124-3435-132.\"\n    \"You can also email me at georgia@coolmail.com\"\n    \"Have a great week!\"\n)\nprint(res)\n</code></pre> Result: <pre><code>{'task_name': 'extract_info', 'llm_model': 'gpt-3.5-turbo-0301', 'template': '{input_instructions}\\n{input_placeholder}\\n{output_instructions}', 'template_args': {'input_instructions': 'Extract the phone number, name and email from the provided text', 'input_placeholder': 'Inputs:\\ntext: {text}\\n', 'output_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\'```json\\' and \\'```\\':\\n```json\\n{{\\n    \"declarai_result\": Dict[str, str]  # The info extracted from the text\\n}}\\n```'}, 'prompt_config': {'structured': True, 'multi_results': False, 'return_name': 'declarai_result', 'temperature': 0.0, 'max_tokens': 2000, 'top_p': 1.0, 'frequency_penalty': 0, 'presence_penalty': 0}, 'call_kwargs': {'text': 'Hey jenny,you can call me at 124-3435-132.You can also email me at georgia@coolmail.comHave a great week!'}, 'result': {'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}, 'time': 2.192906141281128}\n{'phone_number': '124-3435-132', 'name': 'jenny', 'email': 'georgia@coolmail.com'}\n</code></pre></p> <p>We highly recommend you to go through the beginner's guide to get a better understanding of the library and its capabilities - Beginner's Guide</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v012","title":"v0.1.2","text":"<p>View full release on GitHub and PyPi</p> <p>Minor bug fixes</p> <p>Changes:</p> <ul> <li>Updates to documentation</li> <li>Updates to dependencies with reported vulnerabilities</li> <li>Fix typing and improve support for IDE autocompletion</li> <li>Fix issue with initialization failing when passed the <code>openai_token</code> at runtime.</li> </ul>"},{"location":"changelog/#v011","title":"v0.1.1","text":"<p>View full release on GitHub and PyPi</p> <p>Announcing the first release of Declarai! \ud83e\udd73 \ud83e\udd73</p> <p>Declarai was born out of the awe and excitement of LLMs, along with our passion for excellent engineering and real-world applications at scale.</p> <p>We hope this project will help introduce more developers into the world of LLMs and enable them to more easily and reliably integrate these amazing capabilities into their production systems.</p> <p>Main features:</p> <ul> <li>Task interface</li> <li>Chat interface</li> <li>Middlewares</li> <li>Exhaustive documentation</li> <li>OpenAI support</li> <li>LLM prompt best practices</li> </ul>"},{"location":"contribute/","title":"Contribute","text":"<p>Do you like Declarai?</p> <p>Spread the word!</p> <ul> <li>Star  the repository</li> <li>Share the link to the repository with your friends and colleagues</li> <li>Watch the github repository to get notified about new releases.</li> </ul>"},{"location":"contribute/#development","title":"Development","text":"<p>Once you have cloned the repository, install the requirements:</p> <p>Using <code>venv</code></p> PoetryVenv <pre><code>poetry install\n</code></pre> <pre><code>python -m venv env\nsource env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"},{"location":"contribute/#documentation","title":"Documentation","text":"<p>The documentation is built using MkDocs. To view the documentation locally, run the following command:</p> <pre><code>$ cd docs\n$ mkdocs serve\nINFO    -  [11:37:30] Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"contribute/#testing","title":"Testing","text":"<p>The testing framework used is pytest. To run the tests, run the following command:</p> <pre><code>pytest --cov=src   </code></pre>"},{"location":"contribute/#pull-requests","title":"Pull Requests","text":"<p>It should be extermly easy to contribute to this project. If you have any ideas, just open an pull request and we will discuss it.</p> <pre><code>git checkout -b my-new-feature\ngit commit -am 'Add some feature'\ngit push origin my-new-feature\n</code></pre>"},{"location":"newsletter/","title":"Newsletter","text":"<p>Subscribe to our newsletter to stay up to date with the latest news about the declarai, and other cool stuff \ud83d\udcec</p> Subscribe * indicates required Email Address *Company / Organization Full name           /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */          <p></p>"},{"location":"beginners-guide/","title":"Tutorial - Beginners guide","text":"<p>This tutorial is a step-by-step guide to using Declarai. It walks you through the most basic features of the library.</p> <p>Each section gradually builds on the previous one while sections are structured by topic,  so that you can skip to whichever part is relevant to you. </p>"},{"location":"beginners-guide/#before-we-start","title":"Before we start","text":"<p>If you haven't already, install the Declarai library as follows:</p> <pre><code>$ pip install declarai\n</code></pre> <p>Info</p> <p>For this tutorial you will need an openai token. This token is completely your's and is not shared, stored or managed anywhere but on your machine! you can see more information about obtaining a token here: openai</p> <p>After installation, open a python file and start with setting up your declarai app:</p> <p>Once completed, the rest of the examples in this module should be as simple as copy/paste.</p> declarai_tutorial.py<pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\", openai_token=\"&lt;your-openai-token&gt;\")\n</code></pre> <p>Info</p> <p>Do your best to copy, run and edit the code in your editor to really understand how powerful Declarai is.</p>          Lets go!"},{"location":"beginners-guide/#advanced","title":"Advanced","text":"<p>If you feel this tutorial is too easy, feel free to jump to our Features section, which covers more complex  topics like middlewares, running evaluations and building multi provider flows.</p> <p>We recommend you read the tutorial first, and then the advanced guide if you want to learn more.</p>"},{"location":"beginners-guide/controlling-task-behavior/","title":"Controlling task behavior","text":"<p>Task behavior can be controlled by any of the available interfaces in Python. Controlling these parameters is key to achieving the desired results from the model.</p>"},{"location":"beginners-guide/controlling-task-behavior/#passing-parameters-to-the-task","title":"Passing parameters to the task","text":"<p>In the following example, we'll create a task that suggests movies to watch based on a given input. <pre><code>@declarai.task\ndef movie_recommender(user_input: str): # (1)!\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>Notice how providing a type hint for the <code>user_input</code> parameter allows declarai to understand the expected input type.</li> <li>Adding the param to the docstring allows declarai to communicate the meaning of this parameter to the model.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; 'Interstellar'\n</code></pre>"},{"location":"beginners-guide/controlling-task-behavior/#using-return-types-to-control-the-output","title":"Using return types to control the output","text":"<p>This is a good start,  but let's say we want to have a selection of movies instead of a single suggestion. <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]: # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of movie recommendations\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>Adding a return type hint allows declarai to parse the output of the llm into the provided type,     in our case a list of strings.</li> <li>Explaining the return value aids the model in returning the expected output and avoiding hallucinations.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt; ['Interstellar', 'Gravity', 'The Martian', 'Apollo 13', '2001: A Space Odyssey', 'Moon', 'Sunshine', 'Contact', 'The Right Stuff', 'Hidden Figures']\n</code></pre> <p>Info</p> <p>Notice How the text in our documentation has changed from singular to plural form.  Maintaining consistency between the task's description and the return type is important for the model to understand the expected output. For more best-practices, see here. </p> <p>Awesome! </p> <p>Now we have a list of movies to choose from!</p> <p>But what if we want to go even further ?  Let's say we want the model to also provide a short description of each movie. <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]: # (1)!\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\" # (2)!\n</code></pre></p> <ol> <li>We've updated the return value to allow for the creation of a dictionary of movie names and descriptions.</li> <li>We re-enforce the description of the return value to ensure the model understands the expected output.</li> </ol> <pre><code>print(movie_recommender(user_input=\"I want to watch a movie about space\"))\n&gt;{\n   'Interstellar': \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\", \n   'Gravity': 'Two astronauts work together to survive after an accident leaves them stranded in space.', \n   'The Martian': 'An astronaut is left behind on Mars after his team assumes he is dead and must find a way to survive and signal for rescue.', \n   'Apollo 13': 'The true story of the Apollo 13 mission, where an explosion in space jeopardizes the lives of the crew and their safe return to Earth.', \n   '2001: A Space Odyssey': \"A journey through human evolution and the discovery of a mysterious black monolith that may hold the key to humanity's future.\"\n}\n</code></pre> <p>Info</p> <p>A good practice for code readability as well as great performing models is to use type hints and context in the docstrings. The better you describe the task, <code>:params</code> and <code>:return</code> sections within the docstring, the better the results will be.</p> <p>Tip</p> <p>Try experimenting with various descriptions and see how far you can push the model's understanding! who knows what you'll find !</p>          Previous           Next"},{"location":"beginners-guide/debugging-tasks/","title":"Debugging tasks","text":"<p>So it all seems pretty magical up to this point, but what if you want to see what's going on behind the scenes? Being able to debug your tasks is a very important part of the development process, and Declarai makes it easy for you.</p>"},{"location":"beginners-guide/debugging-tasks/#compiling-tasks","title":"Compiling tasks","text":"<p>The first and simplest tool to better understand what's happening under the hood is the <code>compile</code> method. Declarai has an <code>evals</code> module as well for advanced debugging and benchmarking which you can review later here: evals</p> <p>Let's take the last task from the previous section and add a call to the <code>compile</code> method: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n</code></pre> <pre><code>print(movie_recommender.compile())\n\n&gt; {\n    'messages': [ # (1)!\n        # (2)!        \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        # (3)!\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\n              Inputs: user_input: {user_input} # (4)!\n]}\n</code></pre></p> <ol> <li>As we are working with the openai llm provider, which exposes a chat interface, we translate the task into messages as defined by openai's API.</li> <li>In order to guide the task with the correct output format, we provide a system message that explains LLM's role and expected responses</li> <li>The user message is the actual translation of the task at hand, with the user's input as a placeholder for the actual value.</li> <li>{user_input} will be populated with the actual value when the task is being called at runtime.</li> </ol> <p>What we're seeing here is the template for this specific task. It is built so that when called at runtime,  it will be populated with the real values passed to our task.</p> <p>Warning</p> <p>As you can see, that the actual prompt being sent to the model is a bit different than the original docstring.  Even though Declarai incorporates best practices for prompt engineering while maintaining as little interference as possible with user prompts,   it is still possible that the model will not generate the desired output. For this reason it is important to be able to debug your tasks and understand what actually got sent to the model</p>"},{"location":"beginners-guide/debugging-tasks/#compiling-tasks-with-real-values","title":"Compiling tasks with real values","text":"<p>The <code>compile</code> method can also be used to view the prompt with the real values provided to the task. This is useful when prompts might behave differently for different inputs.</p> <pre><code>print(movie_recommender.compile(user_input=\"I want to watch a movie about space\"))\n\n&gt; {\n    'messages': [     \n        system: You are a REST api endpoint. \n                You only answer in JSON structures with a single key named 'declarai_result', nothing else. \n                The expected format is: \"declarai_result\": Dict[string, string]  # A dictionary of movie names and descriptions,\n        user: Recommend a selection of movies to watch based on the user input  \n              For each movie provide a short description as well.\nInputs: user_input: I want to watch a movie about space # (1)!\n]}\n</code></pre> <ol> <li>The actual value of the parameter is now populated in the placeholder and we have our final prompt!</li> </ol> <p>Tip</p> <p>With the <code>compile</code> method, you can always take your prompts anywhere you like,   if it's for monitoring, debugging or just for documentation, we've got you covered!</p>          Previous           Next"},{"location":"beginners-guide/recap/","title":"Recap","text":"<p>In this tutorial you've covered the basics of Declarai! You should now be able to easily:</p> <ul> <li>Create a declarai task.</li> <li>control your task's behavior with native python</li> <li>Use the <code>compile</code> method to view and debug your task template and final prompt!</li> </ul>          Previous"},{"location":"beginners-guide/recap/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"beginners-guide/simple-task/","title":"Simple task","text":"<p>The simplest Declarai usage is a function decorated with <code>@declarai.task</code>:</p> <p><pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\nprint(say_something())\n\n&gt; \"Spread love and kindness to make the world a better place.\"\n</code></pre> In Declarai, The docstring represents the task's description and is used to generate the prompt.</p> <p>By explaining what you want the task to do, the model will be able to understand it and reply with the proper result.</p>          Next"},{"location":"best-practices/","title":"Best practices","text":"<p>Prompt engineering is no simple task and there are various things to consider when creating a prompt. In this page we provide our view and understanding of the best practices for prompt engineering. These will help you create reliably performing tasks and chatbots that won't surprise you when deploying in production.</p> <p>Warning</p> <p>While this guide will should help in creating reliable prompts for most cases, it is still possible that the model will not generate the desired output.  For this reason we strongly recommend you test your tasks and bots on various inputs before deploying to production.  You can acheive this by writing integration tests or using our provided <code>evals</code> library to discover which models and wich  versions perform best for your specific use case.</p>"},{"location":"best-practices/#explicit-is-better-than-implicit","title":"Explicit is better than implicit","text":"<p>When creating a prompt, it is important to be as explicit as possible. Declarai provide various interfaces to provide context and guidance to the model.</p> <p>Reviewing the movie recommender example from the beginner's guide, we can see a collection of techniques to provide context to the model: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    For each movie provide a short description as well\n    :param user_input: The user's input\n    :return: A dictionary of movie names and descriptions\n    \"\"\"\n</code></pre></p> <p>Using type annotations in the input and output create predictability in software and enforce a strict interface with the model. The types are read and enforced by Declarai at runtime so that a produced result of the wrong type will raise an error instead of returned and causing unexpected behavior down the line.</p> <p>Docstrings are used to provide context to the model and to the user.</p> <ul> <li> <p>Task description - The first part of the docstring is the task itself, make sure to address the expected inputs and how to use them     You can implement various popular techniques into the prompt such as <code>few-shot</code>, which means providing example inputs and outputs for the model to learn from.</p> </li> <li> <p>Param descriptions - Explaining the meaning of the input parameters helps the model better perform with the provided inputs.     For example. when passing an argument called <code>input</code>, if you know that the expected input will be an email, or user message, it is best to explain this to the model.</p> </li> <li> <p>Return description - While typing are a great base layer for declaring the expected output,      explaining the exact structure and logic behind this structure will help the model better perform.     For example, given a return type of <code>Dict[str, str]</code>, explaining that this object will contain a mapping of movie names to their respective description      will help to model properly populate the resulting object.</p> </li> </ul>"},{"location":"best-practices/#language-consistency-and-ambiguity","title":"Language consistency and ambiguity","text":"<p>When providing prompts to the model, it is best practice to use language that correlates with the expected input and output. For example, in the following, the prompt is written in single form, while the resulting output is in plural form. (i.e. a list) <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a movie to watch based on the user input\n    :param user_input: The user's input\n    :return: Recommended movie\n    \"\"\"\n</code></pre> This may easily confuse the model and cause it to produce unexpected results which will fail when parsing the results. Instead, we could write the prompt as follows: <pre><code>@declarai.task\ndef movie_recommender(user_input: str) -&gt; List[str]:\n\"\"\"\n    Recommend a selection of movies to watch based on the user input\n    :param user_input: The user's input\n    :return: A list of recommended movies\n    \"\"\"\n</code></pre> This way it is clear to the model that we are expecting a list of movies and not a single movie.</p>"},{"location":"best-practices/#falling-back-to-string","title":"Falling back to string","text":"<p>In some cases, you might be working on a task or chat that has a mixture of behaviors that may not be consistent. For example in this implementation of a calculator bot, the bot usually returns numbers, but for the scenario that an error occurs, it returns a string. <pre><code>@declarai.experimental.chat\nclass CalculatorBot:\n\"\"\"\n    You a calculator bot,\n    given a request, you will return the result of the calculation\n    If you have a problem with the provided input, you should return an error explaining the problem.\n    For example, for the input: \"1 + a\" where 'a' is unknown to you, you should return: \"Unknown symbol 'a'\"\n    \"\"\"\n    def send(self, message: str) -&gt; Union[str, int]:\n        ...\n</code></pre> When using the created bot it should look like this: <pre><code>calc_bot = CalculatorBot()\nprint(calc_bot.send(message=\"1 + 3\"))\n#&gt; 4\nprint(calc_bot.send(message=\"34 * b\"))\n#&gt; Unknown symbol 'b'\n</code></pre> This way, instead of raising an error, the bot returns a string that explains the problem and allows the user to recover from the 'broken' state.</p>"},{"location":"examples/deployments/","title":"Deployments \u2692\ufe0f","text":"<p>Ready to deploy your code? Here are some resources to help you get started:</p>"},{"location":"examples/deployments/#fastapi","title":"FastAPI","text":"<p>Deploying business logic as a REST API is a common pattern. FastAPI is an ultimate solution. Here's how you can deploy your Declarai code behind a REST API using FastAPI:</p> <pre><code>from typing import Dict\nfrom pydantic import BaseModel\nfrom fastapi import FastAPI, APIRouter\nfrom declarai import Declarai\n\napp = FastAPI()\nrouter = APIRouter()\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n\n@declarai.task\ndef movie_recommender(user_input: str) -&gt; Dict[str, str]:\n\"\"\"\n    Recommend a selection of real movies to watch based on the user input\n    For each movie provide its name and a short description as well.\n    :param user_input: The user's input\n    :return: A mapping between movie names and descriptions\n    \"\"\"\n\n\nclass MovieRecommendationRequest(BaseModel):\n    user_input: str\n\n\n@router.post(\"/movie_recommender\", response_model=Dict[str, str])\ndef run_movie_recommender(request: MovieRecommendationRequest) -&gt; Dict[str, str]:\n\"\"\"\n    Run the movie recommender task behind a post request\n    \"\"\"\n    return movie_recommender(user_input=request.user_input)\n\n\napp.include_router(router)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app)\n</code></pre> <p>You can now run the server with <code>python app.py</code> and send a POST request to <code>http://localhost:8000/movie_recommender</code>:</p> <pre><code>import requests\n\nres = requests.post(\"http://localhost:8000/movie_recommender\",\n              json={\"user_input\": \"I want to watch a movie about space\"})\n&gt;&gt;&gt; res.json()\n\n{'Gravity': 'Two astronauts work together to survive after an accident leaves '\n            'them stranded in space.',\n 'Interstellar': 'A team of explorers travel through a wormhole in space in an '\n                 \"attempt to ensure humanity's survival.\",\n 'The Martian': 'An astronaut is left stranded on Mars and must find a way to '\n                'survive until rescue is possible.'}\n</code></pre>"},{"location":"examples/deployments/#streamlit-app","title":"Streamlit app","text":"<p>Streamlit is a great tool for quickly building interactive web apps. Assuming you have deployed your Declarai code as a REST API, you can use the following snippet to build a Streamlit app that interacts with it: <pre><code>import streamlit as st\nimport requests\n\nBACKEND_URL = \"http://localhost:8000\"\nst.title(\"Welcome to Movie Recommender System\")\nst.write(\"This is a demo of a movie recommender system built using Declarai\")\n\nuser_input = st.text_input(\"What kind of movies do you like?\")\nbutton = st.button(\"Submit\")\nif button:\n    print(user_input)\n    with st.spinner(\"Thinking..\"):\n        res = requests.post(f\"{BACKEND_URL}/movie_recommender\", json={\"user_input\": user_input})\n    st.write(res.json())\n</code></pre> </p>"},{"location":"features/","title":"Features","text":"<p>As Declarai is aimed at being completely extensible and configurable, we provide interfaces to override and interact with any of the default behaviours if you choose.</p> <p>We are still actively working on exposing all the necessary interfaces to make this possible, so if there are any interfaces you would like to see exposed, please vote or open an issue on our GitHub</p>"},{"location":"features/language-model-parameters/","title":"Control LLM params","text":"<p>Language models have various parameters that can be tuned to control the output of the model. To see the parameters for a specific LLM, see the corresponding provider.</p> <p>Here is an example of how to control these parameters in a declarai task/chat:</p>"},{"location":"features/language-model-parameters/#set-at-declaration","title":"Set at declaration","text":"<pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n\n@declarai.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n</code></pre>"},{"location":"features/language-model-parameters/#set-at-runtime","title":"Set at runtime","text":"<p>We can also pass parameters to the declarai task/chat interface at runtime:</p> <pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n\n@declarai.task\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n\ngenerate_song(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000}) # (1)!\n</code></pre> <ol> <li>The <code>llm_params</code> argument is passed at runtime instead of at declaration.</li> </ol>"},{"location":"features/language-model-parameters/#override-at-runtime","title":"Override at runtime","text":"<p>Furthermore, we can pass parameters to the declarai task/chat interface at runtime and override the parameters passed at declaration:</p> <pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n\n@declarai.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n\ngenerate_song(llm_params={\"temperature\": 0.3, \"max_tokens\": 500})\n</code></pre> <p>In this case, the <code>llm_params</code> argument passed at runtime will override the <code>llm_params</code> argument passed at declaration.</p>"},{"location":"features/language-model-parameters/#set-for-chat-interface","title":"Set for Chat interface","text":"<p>Same as with tasks, we can pass parameters to the declarai chat interface at declaration, at runtime, or override the parameters passed at declaration at runtime.</p> <p><pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n\n@declarai.experimental.chat(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000})\nclass SQLAdvisor:\n\"\"\"\n    You are a proficient sql adivsor.\n    Your goal is to help user's with sql related questions.\n    \"\"\"\n\nsql_advisor = SQLAdvisor()\n</code></pre> In the case above, all messages sent to the chat interface will use the parameters passed at declaration.</p>"},{"location":"features/magic/","title":"Magic","text":"<p>The Magic callable is an \"empty\" function that can be used for two main scenarios:</p> <ul> <li>A placeholder for typing, so to simplify interaction with static typing without hacing to mark all Declarai functions with <code># type: ignore</code>:</li> <li>A replacement for the docstring content, if for some reason you don't want to use the docstring for the task description.</li> </ul>"},{"location":"features/magic/#magic-as-a-placeholder-for-typing","title":"Magic as a placeholder for typing","text":"<p>Without magic: <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str: # (1)!\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n</code></pre></p> <ol> <li>type hinter warning on unused argument <code>real_name</code> in function.</li> </ol> <p>with magic: <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str:\n\"\"\"\n    Suggest a nickname for a person\n    :param real_name: The person's real name\n    :return: A nickname for the person\n    \"\"\"\n    return declarai.magic(real_name=real_name) # (1)!\n</code></pre></p> <ol> <li>type hint warning is resolved.</li> </ol>"},{"location":"features/magic/#replacement-for-docstring","title":"Replacement for docstring","text":"<p>In the scenario that you do not wan't to rely on the docstring for prompt generation, you can use the magic function to provide the description and parameters.</p> <pre><code>@declarai.task\ndef suggest_nickname(real_name: str) -&gt; str:\n    return declarai.magic(\n        real_name=real_name,\n        description=\"Suggest a nickname for a person\",\n        params={\"real_name\": \"The person's real name\"},\n        returns=\"A nickname for the person\",\n    )\n</code></pre> <p>This does take some of Declarai's magic out of the equation, but the result should be all the same.</p>"},{"location":"features/multi-model-multi-provider/","title":"Multiple models / Multiple providers","text":"<p>Declarai allows you to use multiple models from different providers in the same project. All you need to do is configure seperate Declarai instances for each model and provider.</p> <pre><code>from declarai import Declarai\n\n# Configure the first Declarai instance\ndeclarai_gpt35 = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n# Configure the second Declarai instance\ndeclarai_gpt4 = Declarai(provider=\"openai\", model=\"gpt-4\")\n\n# Now use the instances to create tasks\n@declarai_gpt35.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n\n@declarai_gpt4.task\ndef say_something() -&gt; str:\n\"\"\"\n    Say something short to the world\n    \"\"\"\n</code></pre>"},{"location":"features/planning-future-tasks/","title":"Planning future tasks","text":""},{"location":"features/planning-future-tasks/#plan-task","title":"Plan task","text":"<p>Once you have defined your task, you can create a plan for it that is already populated with the real values of the parameters.</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. </p> <pre><code>from declarai import init_declarai, magic\n\ntask = init_declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@task\ndef say_something_about_movie(movie: str) -&gt; str:  \n\"\"\"\n    Say something short about the following movie\n    :param movie: The movie name\n    \"\"\"\n\n    return magic(movie)\n\nplan = say_something_about_movie.plan(movie=\"Avengers\")\n\nprint(plan)\n&gt; #&lt;declarai.tasks.base_llm_task.LLMTaskFuture object at 0x106795790&gt;\n\n\n# Execute the task by calling the plan\nplan()\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n   \"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre> <p>Important</p> <p>The plan is an object you call and get the results. This is very helpful when you want to populate the task with the real values of the parameters but delay the execution of it. If you just want to execute the task, you can call the task directly.</p> <pre><code>res = say_something_about_movie(movie=\"Avengers\")\n\n&gt; ['I liked the action-packed storyline and the epic battle scenes.',\n\"I didn't like the lack of character development for some of the Avengers.\"]\n</code></pre>"},{"location":"features/chat/","title":"Chatbots","text":"<p>Unlike tasks, chatbots are meant to keep the conversation going.  Instead of executing a single operation, they are built to manage conversation context over time.</p> <p>Declarai can be used to create chatbots. The simplest way to do this is to use the <code>@declarai.experimental.chat</code> decorator.</p> <p>We declare a \"system prompt\" in the docstring of the class definition. The system prompt is the initial command that instructs the bot on who they are and what's expected in the conversation. </p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions \n    \"\"\" # (1)!\n</code></pre> <ol> <li>The docstring represents the chatbot's description and is used to generate the prompt.</li> </ol> <pre><code>sql_bot = SQLBot()\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n\n&gt; \"You should use a LEFT JOIN when you want to return all rows from the left table, and the matched rows from the right table.\"\n</code></pre> <ol> <li>The created bot exposes a <code>send</code> method, by which you can interact and send messages.     Every call to send results with a response from the bot.</li> </ol> <p>Tip</p> <p>You can also declare the chatbot system prompt by doing the following <pre><code>@declarai.experimental.chat\nclass SQLBot:\n    pass\nsql_bot = SQLBot(system=\"You are a sql assistant. You help with SQL related questions with one-line answers.\")\n</code></pre></p>"},{"location":"features/chat/advanced-initialization/","title":"Initialization","text":"<p>Although using the docstring and class properties is the recommended way to initialize a chatbot, it is not the only way. In cases were relying on the class docstring and properties is problematic, we allow manually passing the chat arguments to the class constructor. This takes away from the magic that Declarai provides, but we are aware not everyone may be comfortable with it.</p>"},{"location":"features/chat/advanced-initialization/#initialization-by-passing-parameters","title":"Initialization by passing parameters","text":"<p>Let's see how we can initialize a chatbot by passing the <code>system</code> and <code>greeting</code> parameters as arguments.</p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n    ...\n\n\nsql_bot = SQLBot(\n    system=\"You are a sql assistant. You help with SQL queries with one-line answers.\",\n    greeting=\"Hello, I am a SQL assistant. How can I assist you today?\",\n)\n\nprint(sql_bot.send(\"Tell me your preferred SQL operation\"))\n</code></pre> <pre><code>&gt; \"As an SQL assistant, I don't have a preferred SQL operation. I am here to assist with any SQL operation you need help with.\"\n</code></pre>"},{"location":"features/chat/advanced-initialization/#next-steps","title":"Next steps","text":"<p>You are welcome to explore our Features section, where you can find the full list of supported features and how to use them.</p>"},{"location":"features/chat/controlling-chat-behavior/","title":"Controlling chat behavior","text":""},{"location":"features/chat/controlling-chat-behavior/#greetings","title":"Greetings","text":"<p>Greetings are used to start the conversation with a bot message instead of a user message. The <code>greeting</code> attribute defines this first message and is added to the conversation on initialization.</p> <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre> <p>The greeting attribute is later available as a property of the chatbot instance to use when implementing your interface. <pre><code>sql_bot = SQLBot()\nsql_bot.greeting\n\n&gt; \"Hello, I am a SQL assistant. How can I assist you today?\"\n</code></pre></p> <pre><code>sql_bot.send(\"When should I use a LEFT JOIN?\")\n\n&gt; 'You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.'\n\nsql_bot.conversation\n\n&gt; [ # (1)!\n    assistant: Hello, I am a SQL assistant. How can I assist you today?,\n    user: When should I use a LEFT JOIN?,\n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\n] \n</code></pre> <ol> <li>We can see here that the greeting, initiated by the assistant, is the first message in the conversation.</li> </ol>"},{"location":"features/chat/controlling-chat-behavior/#inject-a-message-to-the-memory","title":"Inject a message to the memory","text":"<p>Declarai enables injecting custom messages into the conversation history by using the <code>add_message</code> method.</p> <p>This is super useful when you want to intervene with the conversation flow without necessarily triggering another response from the model.</p> <p>Consider using it for:  </p> <ul> <li>Creating a prefilled conversation even before the user's interaction.  </li> <li>Modifying the chatbot memory after the chatbot has generated a response.  </li> <li>Modifying the chatbot system prompt.</li> <li>Guiding the conversation flow given certain criteria met in the user-bot interaction.</li> </ul> <pre><code>sql_bot = SQLBot()\nsql_bot.add_message(\"From now on, answer I DONT KNOW on any question asked by the user\", role=\"system\") \n# (1)!\nsql_bot.send(\"What is your favorite SQL operation?\")\n\n&gt; \"I don't know.\"\n</code></pre> <ol> <li>The chatbot's conversation history now contains the injected message and reacts accordingly.</li> </ol>"},{"location":"features/chat/controlling-chat-behavior/#dynamic-system-prompting","title":"Dynamic system prompting","text":"<p>In the following example, we will pass a parameter to the chatbot system prompt. This value will be populated at runtime and will allow us to easily create base chatbots with varying behaviors.</p> <pre><code>@declarai.experimental.chat\nclass JokeGenerator:\n\"\"\"\n    You are a joke generator. You generate jokes that a {character} would tell.\n    \"\"\" # (1)!\n\n\ngenerator = JokeGenerator()\nfavorite_joke = generator.send(character=\"Spongebob\", message=\"What is your favorite joke?\")\nsquidward_joke = generator.send(message=\"What jokes can you tell about squidward?\")\n\nprint(favorite_joke)\nprint(squidward_joke)\n</code></pre> <ol> <li>The system prompt now contains the parameter <code>{character}</code>. This parameter will be replaced by the value passed to the <code>send</code> method.</li> </ol> <pre><code>&gt; \"Why did the jellyfish go to school? Because it wanted to improve its \"sting-uage\" skills!\"\n&gt; \"Why did Squidward bring a ladder to work? Because he wanted to climb up the corporate \"sour-cules\"!\"\n</code></pre>"},{"location":"features/chat/customizing-chat-response/","title":"Customizing the Chat Response","text":"<p>The default response type of the language model messages is <code>str</code>. However, you can overwrite the <code>send</code> method to return a different type. Just like tasks, you can control the type hints by declaring the return type of the <code>send</code> method.</p> <pre><code>from typing import List\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant.\"\"\"\n    ...\n\n    def send(self, operation: str) -&gt; List[str]:\n        ...\n\nsql_bot = SQLBot()\nprint(sql_bot.send(message=\"Offer two sql queries that use the 'SELECT' operation\"))\n&gt; [\n    \"SELECT * FROM table_name;\",\n    \"SELECT column_name FROM table_name;\"\n]\n</code></pre> <p>Warning</p> <p>As with tasks, the message is sent along with the expected return types.  This means that if not careful, a message conflicting with the expected results could cause weird behavior in the llm responses.  For more best-practices, see here.</p>"},{"location":"features/chat/debugging-chat/","title":"Debugging Chat","text":"<p>Similarly to debugging tasks, understanding the prompts being sent to the llm is crucial to debugging chatbots. Declarai exposes the <code>compile</code> method for chat instances as well!</p>"},{"location":"features/chat/debugging-chat/#compiling-chat","title":"Compiling chat","text":"<p><pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL queries with one-line answers.\n    \"\"\"\n    greeting = \"Hello, I am a SQL assistant. How can I assist you today?\"\n\nsql_bot = SQLBot()\nprint(sql_bot.compile())\n</code></pre> <pre><code>&gt; {\n    'messages': \n        [\n            \"system: You are a sql assistant. You help with SQL queries with one-line answers.\", \n            \"assistant: Hello, I am a SQL assistant. How can I assist you today?\"\n        ]\n}\n</code></pre> Wonderful right? We can view the chatbot's messages in the format they will be sent to the language model.</p>"},{"location":"features/chat/chat-memory/","title":"Chat memory","text":"<p>A chat instance saves the message history and uses it to future responses. Here is an example of a chatbot that retains conversation history across multiple <code>send</code> requests. <pre><code>@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n\nsql_bot.send(\"When should I use a LEFT JOIN?\") # (1)!\n&gt; \"You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.\"\n\nsql_bot.send(\"But how is it different from a RIGHT JOIN?\") # (2)!\n&gt; \"A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\"\n</code></pre></p> <ol> <li>The first message is sent with the system prompt.</li> <li>The second message is sent with the previous conversation and therefore the model is aware of the first question.</li> </ol>"},{"location":"features/chat/chat-memory/#conversation-history","title":"Conversation History","text":"<p>You can view the conversation history by accessing the <code>conversation</code> attribute.</p> <pre><code>sql_bot.conversation\n\n&gt; [\n    user: When should I use a LEFT JOIN?, \n    assistant: You should use a LEFT JOIN when you want to retrieve all records from the left table and matching records from the right table.,\n    user: But how is it different from a RIGHT JOIN?,\n    assistant: A LEFT JOIN retrieves all records from the left table and matching records from the right table, while a RIGHT JOIN retrieves all records from the right table and matching records from the left table.\n]\n</code></pre> <p>Warning</p> <p>Keep in mind that the conversation history does not contain the system prompt. It only contains the user messages and the chatbot responses.</p> <p>If you want to access the system message, you can use the <code>system</code> attribute.</p> <pre><code>sql_bot.system\n\n&gt; \"system: You are a sql assistant. You help with SQL related questions with one-line answers.\\n\"\n</code></pre>"},{"location":"features/chat/chat-memory/#default-memory","title":"Default Memory","text":"<p>The default message history of a chat is a simple in-memory list. This means that history exists only for the duration of the chatbot session.</p> <p>If you prefer to have a persistent history, you can use the <code>FileMessageHistory</code> class from the <code>declarai.memory</code> module.</p>"},{"location":"features/chat/chat-memory/#setting-up-a-memory","title":"Setting up a memory","text":"<p>Setting up a memory is done by passing <code>chat_history</code> as a keyword argument to the <code>declarai.experimental.chat</code> decorator.</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import FileMessageHistory\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat(chat_history=FileMessageHistory(\"sql_bot_history.txt\")) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n</code></pre> <ol> <li>file path is not mandatory. If you do not provide a file path, the default file path is stored in a tmp directory.</li> </ol> <p>We can also initialize the chat_history at runtime</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import FileMessageHistory\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\nsql_bot = SQLBot(chat_history=FileMessageHistory(\"sql_bot_history.txt\"))\n</code></pre>"},{"location":"features/chat/chat-memory/file-memory/","title":"File Memory","text":"<p>For chat that requires a persistent message history, you can use a file to store the conversation history.</p>"},{"location":"features/chat/chat-memory/file-memory/#set-file-memory","title":"Set file memory","text":"<pre><code>from declarai import Declarai\nfrom declarai.memory import FileMessageHistory\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat(chat_history=FileMessageHistory(\"sql_bot_history.txt\")) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>file path is not mandatory. If you do not provide a file path, the default file path is stored in a tmp directory.</li> </ol> <p>We can also initialize the <code>FileMessageHistory</code> class with a custom file path.</p>"},{"location":"features/chat/chat-memory/file-memory/#set-file-memory-at-runtime","title":"Set file memory at runtime","text":"<p>In case you want to set the file memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import FileMessageHistory\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=FileMessageHistory(\"sql_bot_history.txt\"))\n</code></pre>"},{"location":"features/chat/chat-memory/mongodb-memory/","title":"MongoDB Memory","text":"<p>For chat that requires a persistent and scalable message history, you can use a MongoDB database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/mongodb-memory/#set-mongodb-memory","title":"Set MongoDB memory","text":"<pre><code>from declarai import Declarai\nfrom declarai.memory import MongoDBMessageHistory\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat(\n    chat_history=MongoDBMessageHistory(\n        connection_string=\"mongodb://localhost:27017/mydatabase\",\n        session_id=\"unique_chat_id\")\n) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>connection_string</code> parameter specifies the connection details for the MongoDB database. Replace <code>localhost</code>, <code>27017</code>, and <code>mydatabase</code> with your specific MongoDB connection details. The <code>session_id</code> parameter uniquely identifies the chat session for which the history is being stored.</li> </ol>"},{"location":"features/chat/chat-memory/mongodb-memory/#set-mongodb-memory-at-runtime","title":"Set MongoDB memory at runtime","text":"<p>In case you want to set the MongoDB memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import MongoDBMessageHistory\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\n\nsql_bot = SQLBot(chat_history=MongoDBMessageHistory(connection_string=\"mongodb://localhost:27017/mydatabase\",\n                                                   session_id=\"unique_chat_id\"))\n</code></pre>"},{"location":"features/chat/chat-memory/mongodb-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using MongoDB memory.</p> <pre><code>pip install declarai[mongodb]\n</code></pre>"},{"location":"features/chat/chat-memory/postgresql-memory/","title":"PostgreSQL Memory","text":"<p>For chat that requires a persistent message history with the advantages of scalability and robustness, you can use a PostgreSQL database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/postgresql-memory/#set-postgresql-memory","title":"Set PostgreSQL memory","text":"<pre><code>from declarai import Declarai\nfrom declarai.memory import PostgresMessageHistory\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat(\n    chat_history=PostgresMessageHistory(\n        connection_string=\"postgresql://username:password@localhost:5432/mydatabase\",\n        session_id=\"unique_chat_id\")\n) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>connection_string</code> parameter specifies the connection details for the PostgreSQL database. Replace <code>username</code>, <code>password</code>, <code>localhost</code>, <code>5432</code>, and <code>mydatabase</code> with your specific PostgreSQL connection details. The <code>session_id</code> parameter uniquely identifies the chat session for which the history is being stored.</li> </ol>"},{"location":"features/chat/chat-memory/postgresql-memory/#set-postgresql-memory-at-runtime","title":"Set PostgreSQL memory at runtime","text":"<p>In case you want to set the PostgreSQL memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import PostgresMessageHistory\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=PostgresMessageHistory(connection_string=\"postgresql://username:password@localhost:5432/mydatabase\", session_id=\"unique_chat_id\"))\n</code></pre>"},{"location":"features/chat/chat-memory/postgresql-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using PostgreSQL memory.</p> <pre><code>pip install declarai[postgresql]\n</code></pre>"},{"location":"features/chat/chat-memory/redis-memory/","title":"Redis Memory","text":"<p>For chat that requires a fast and scalable message history, you can use a Redis database to store the conversation history.</p>"},{"location":"features/chat/chat-memory/redis-memory/#set-redis-memory","title":"Set Redis memory","text":"<pre><code>from declarai import Declarai\nfrom declarai.memory import RedisMessageHistory\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat(\n    chat_history=RedisMessageHistory(\n        session_id=\"unique_chat_id\",\n        url=\"redis://localhost:6379/0\"\n    )\n) # (1)!\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot()\n</code></pre> <ol> <li>The <code>url</code> parameter specifies the connection details for the Redis server. Replace <code>localhost</code> and <code>6379</code> with your specific Redis connection details. The <code>session_id</code> parameter uniquely identifies the chat session for which the history is being stored.</li> </ol> <p>We can also initialize the <code>RedisMessageHistory</code> class with custom connection details.</p>"},{"location":"features/chat/chat-memory/redis-memory/#set-redis-memory-at-runtime","title":"Set Redis memory at runtime","text":"<p>In case you want to set the Redis memory at runtime, you can use the <code>set_memory</code> method.</p> <pre><code>from declarai import Declarai\nfrom declarai.memory import RedisMessageHistory\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\n@declarai.experimental.chat\nclass SQLBot:\n\"\"\"\n    You are a sql assistant. You help with SQL related questions with one-line answers.\n    \"\"\"\n\nsql_bot = SQLBot(chat_history=RedisMessageHistory(session_id=\"unique_chat_id\", url=\"redis://localhost:6379/0\"))\n</code></pre>"},{"location":"features/chat/chat-memory/redis-memory/#dependencies","title":"Dependencies","text":"<p>Make sure to install the following dependencies before using Redis memory.</p> <pre><code>pip install declarai[redis]\n</code></pre>"},{"location":"features/evals/","title":"Evaluations","text":"<p>The <code>evals</code> library is an addition over the base <code>declarai</code> library that provides tools to track and benchmark the performance of prompt strategies across models and providers.</p> <p>We understand that a major challenge in the field of prompt engineering is the lack of a standardised way to evaluate along with the continuously evolving nature of the field. As such, we have designed the <code>evals</code> library to be a lean wrapper over the <code>declarai</code> library that allows users to easily track and benchmark changes in prompts and models.</p>"},{"location":"features/evals/#usage","title":"Usage","text":"<pre><code>$ python -m declarai.evals.evaluator\nRunning Extraction scenarios...\nsingle_value_extraction... \n---&gt; 100%\nmulti_value_extraction...\n---&gt; 100%\nmulti_value_multi_type_extraction...\n---&gt; 100%\n...\nDone!\n</code></pre>"},{"location":"features/evals/#evaluations_1","title":"Evaluations","text":"<p>The output table will allow you to review the performance of your task across models and provides and make an informed decision on which model and provider to use for your task.</p> Provider Model version Scenario runtime output openai gpt-3.5-turbo latest generate_a_poem_no_metadata 1.235s Using LLMs is fun! openai gpt-3.5-turbo 0301 generate_a_poem_no_metadata 0.891s Using LLMs is fun! It's like playing with words Creating models that learn And watching them fly like birds openai gpt-3.5-turbo 0613 generate_a_poem_no_metadata 1.071s Using LLMs is fun! openai gpt-4 latest generate_a_poem_no_metadata 3.494s {'poem': 'Using LLMs, a joyous run,\\nIn the world of AI, under the sun.\\nWith every task, they stun,\\nIndeed, using LLMs is fun!'} openai gpt-4 0613 generate_a_poem_no_metadata 4.992s {'title': 'Using LLMs is fun!', 'poem': \"With LLMs, the fun's just begun, \\nCoding and learning, second to none. \\nComplex tasks become a simple run, \\nOh, the joy when the work is done!\"} openai gpt-3.5-turbo latest generate_a_poem_only_return_type 2.1s Learning with LLMs, a delightful run, Exploring new knowledge, it's never done. With every challenge, we rise and we stun, Using LLMs, the learning is always fun!"},{"location":"integrations/","title":"Integrations","text":"<p>Declarai comes with minimal dependencies out of the box, to keep the core of the library clean and simple. If you would like to extend the functionality of Declarai, you can install one of the following integrations.</p>"},{"location":"integrations/#wandb","title":"Wandb","text":"<p>Weights &amp; Biases is a popular tool for tracking machine learning experiments. Recently they have provided an API for their tracking prompts in their platform. The platform has a free tier which you can use to experiment!</p> <pre><code>pip install declarai[wandb]\n</code></pre> <p>Info</p> <p>To use this integration you will need to create an account at wandb. Once created,   you can create a new project and get your API key from the settings page.</p> <p>Once set up, you can use the <code>WandDBMonitorCreator</code> to track your prompts in the platform.</p> <p><pre><code>from typing import Dict\nfrom declarai import Declarai\nfrom declarai.middlewares.third_party.wandb_monitor import WandDBMonitorCreator\n\n\ndeclarai = Declarai(provider=\"openai\", model=\"gpt-3.5-turbo\")\n\nWandDBMonitor = WandDBMonitorCreator(\n    name=\"&lt;context-name&gt;\",\n    project=\"&lt;project-name&gt;\",\n    key=\"&lt;your-decorators-key&gt;\",\n)\n\n\n@declarai.task(middlewares=[WandDBMonitor])\ndef extract_info(text: str) -&gt; Dict[str, str]:\n\"\"\"\n    Extract the phone number, name and email from the provided text\n    :param text: content to extract the info from\n    :return: The info extracted from the text\n    \"\"\"\n    return Declarai.magic(text=text)\n</code></pre> The tracked prompts should look like this:</p> <p> </p>"},{"location":"providers/","title":"Providers","text":"<p>Declarai supports the following providers:</p> <ul> <li>OpenAI</li> </ul>"},{"location":"providers/openai/","title":"Openai","text":"<p>To use OpenAI models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? API key <code>DECLARAI_OPENAI_API_KEY</code> <code>Declarai(... openai_token=&lt;api-token&gt;)</code> \u2705"},{"location":"providers/openai/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you    will not be able to see it again.</li> </ol>"},{"location":"providers/openai/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>DECLARAI_OPENAI_API_KEY</code>.</p> <p>To establish your OpenAI API key as an environment variable, launch your terminal and execute the following command, substituting  with your actual key: <pre><code>export DECLARAI_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This action will maintain the key for the duration of your terminal session. To ensure a longer retention, modify your terminal's settings or corresponding environment files.</p>"},{"location":"providers/openai/#control-llm-parameters","title":"Control LLM Parameters","text":"<p>OpenAI models have a number of parameters that can be tuned to control the output of the model. These parameters are passed to the declarai task/chat interface as a dictionary. The following parameters are supported:</p> Parameter Type Description Default <code>temperature</code> <code>float</code> Controls the randomness of the model. Lower values make the model more deterministic and repetitive. Higher values make the model more random and creative. <code>0</code> <code>max_tokens</code> <code>int</code> Controls the length of the output. <code>3000</code> <code>top_p</code> <code>float</code> Controls the diversity of the model. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>1</code> <code>frequency_penalty</code> <code>float</code> Controls how often the model repeats itself. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <code>presence_penalty</code> <code>float</code> Controls how often the model generates new topics. Lower values make the model more repetitive and conservative. Higher values make the model more random and creative. <code>0</code> <p>Pass your custom parameters to the declarai task/chat interface as a dictionary:</p> <pre><code>from declarai import Declarai\n\ndeclarai = Declarai(provide=\"openai\", model=\"gpt4\", openai_token=\"&lt;your API key&gt;\")\n\n@declarai.task(llm_params={\"temperature\": 0.5, \"max_tokens\": 1000}) # (1)!\ndef generate_song():\n\"\"\"\n    Generate a song about declarai\n    \"\"\"\n</code></pre> <ol> <li>Pass only the parameters you want to change. The rest will be set to their default values.</li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> declarai<ul> <li> _base</li> <li> chat</li> <li> core<ul> <li> core_settings</li> </ul> </li> <li> declarai</li> <li> memory<ul> <li> base</li> <li> file</li> <li> in_memory</li> <li> mongodb</li> <li> postgres</li> <li> redis</li> </ul> </li> <li> middleware<ul> <li> base</li> <li> internal<ul> <li> log_middleware</li> </ul> </li> <li> third_party<ul> <li> wandb_monitor</li> </ul> </li> </ul> </li> <li> operators<ul> <li> llm</li> <li> message</li> <li> openai_operators<ul> <li> chat_operator</li> <li> openai_llm</li> <li> settings</li> <li> task_operator</li> </ul> </li> <li> operator</li> <li> templates<ul> <li> chain_of_thought</li> <li> instruct_function</li> <li> output_prompt</li> <li> output_structure</li> </ul> </li> </ul> </li> <li> python_parser<ul> <li> docstring_parsers<ul> <li> reST<ul> <li> parser</li> </ul> </li> <li> types</li> </ul> </li> <li> magic_parser</li> <li> parser</li> <li> type_annotation_to_schema</li> <li> types</li> </ul> </li> <li> task</li> </ul> </li> </ul>"},{"location":"reference/declarai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> declarai","text":""},{"location":"reference/declarai/#declarai","title":"declarai","text":"<p>Modules:</p> Name Description <code>task</code> <p>Task</p> <code>declarai</code> <p>This is the main entry point for declarai. It is used to decorate the package functionalities and serve as the main interface for the user.</p> <code>chat</code> <p>Chat tasks are tasks that are meant to be used in an iterative fashion, where the user and the assistant exchange</p>"},{"location":"reference/declarai/_base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> _base","text":""},{"location":"reference/declarai/_base/#declarai._base","title":"_base","text":"<p>Classes:</p> Name Description <code>BaseTask</code> <p>Base class for tasks.</p> <code>BaseChat</code> <p>Base class for chat tasks. Same as <code>BaseTask</code>, but with a <code>BaseChatOperator</code> instead of a <code>BaseOperator</code>.</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat","title":"BaseChat","text":"<p>             Bases: <code>BaseTask</code></p> <p>Base class for chat tasks. Same as <code>BaseTask</code>, but with a <code>BaseChatOperator</code> instead of a <code>BaseOperator</code>. See <code>Chat</code> for default implementation.</p> <p>Methods:</p> Name Description <code>_exec</code> <p>Execute the task</p> <code>_exec_middlewares</code> <p>Execute the task middlewares and the task itself</p> <code>compile</code> <p>Compile the task to get the prompt sent to the LLM</p> <code>__call__</code> <p>Orchestrates the execution of the task</p> <p>Attributes:</p> Name Type Description <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed.</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Orchestrates the execution of the task</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on the inherited class</p> <code>()</code> <code>**kwargs</code> <p>Depends on the inherited class</p> <code>{}</code> Source code in <code>src/declarai/_base.py</code> <pre><code>def __call__(self, *args, **kwargs):\n\"\"\"\n    Orchestrates the execution of the task\n    Args:\n        *args: Depends on the inherited class\n        **kwargs: Depends on the inherited class\n\n    Returns: The result of the task, after parsing the result of the llm.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat._exec","title":"_exec  <code>abstractmethod</code>","text":"<pre><code>_exec(kwargs: dict) -&gt; Any\n</code></pre> <p>Execute the task</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>the runtime keyword arguments that are used to compile the task prompt.</p> required Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec(self, kwargs: dict) -&gt; Any:\n\"\"\"\n    Execute the task\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat._exec_middlewares","title":"_exec_middlewares  <code>abstractmethod</code>","text":"<pre><code>_exec_middlewares(kwargs) -&gt; Any\n</code></pre> <p>Execute the task middlewares and the task itself</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>the runtime keyword arguments that are used to compile the task prompt.</p> required Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec_middlewares(self, kwargs) -&gt; Any:\n\"\"\"\n    Execute the task middlewares and the task itself\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator. Same as `_exec`.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseChat.compile","title":"compile  <code>abstractmethod</code>","text":"<pre><code>compile(**kwargs) -&gt; str\n</code></pre> <p>Compile the task to get the prompt sent to the LLM</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>the runtime keyword arguments that are placed within the prompt string.</p> <code>{}</code> Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef compile(self, **kwargs) -&gt; str:\n\"\"\"\n    Compile the task to get the prompt sent to the LLM\n    Args:\n        **kwargs: the runtime keyword arguments that are placed within the prompt string.\n\n    Returns: The prompt string that is sent to the LLM\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask","title":"BaseTask","text":"<p>Base class for tasks.</p> <p>Methods:</p> Name Description <code>_exec</code> <p>Execute the task</p> <code>_exec_middlewares</code> <p>Execute the task middlewares and the task itself</p> <code>compile</code> <p>Compile the task to get the prompt sent to the LLM</p> <code>__call__</code> <p>Orchestrates the execution of the task</p> <p>Attributes:</p> Name Type Description <code>operator</code> <code>BaseOperator</code> <p>The operator to use for the task</p> <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed.</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.operator","title":"operator  <code>instance-attribute</code>","text":"<pre><code>operator: BaseOperator\n</code></pre> <p>The operator to use for the task</p>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Orchestrates the execution of the task</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Depends on the inherited class</p> <code>()</code> <code>**kwargs</code> <p>Depends on the inherited class</p> <code>{}</code> Source code in <code>src/declarai/_base.py</code> <pre><code>def __call__(self, *args, **kwargs):\n\"\"\"\n    Orchestrates the execution of the task\n    Args:\n        *args: Depends on the inherited class\n        **kwargs: Depends on the inherited class\n\n    Returns: The result of the task, after parsing the result of the llm.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask._exec","title":"_exec  <code>abstractmethod</code>","text":"<pre><code>_exec(kwargs: dict) -&gt; Any\n</code></pre> <p>Execute the task</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>the runtime keyword arguments that are used to compile the task prompt.</p> required Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec(self, kwargs: dict) -&gt; Any:\n\"\"\"\n    Execute the task\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask._exec_middlewares","title":"_exec_middlewares  <code>abstractmethod</code>","text":"<pre><code>_exec_middlewares(kwargs) -&gt; Any\n</code></pre> <p>Execute the task middlewares and the task itself</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>the runtime keyword arguments that are used to compile the task prompt.</p> required Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec_middlewares(self, kwargs) -&gt; Any:\n\"\"\"\n    Execute the task middlewares and the task itself\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator. Same as `_exec`.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/_base/#declarai._base.BaseTask.compile","title":"compile  <code>abstractmethod</code>","text":"<pre><code>compile(**kwargs) -&gt; str\n</code></pre> <p>Compile the task to get the prompt sent to the LLM</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>the runtime keyword arguments that are placed within the prompt string.</p> <code>{}</code> Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef compile(self, **kwargs) -&gt; str:\n\"\"\"\n    Compile the task to get the prompt sent to the LLM\n    Args:\n        **kwargs: the runtime keyword arguments that are placed within the prompt string.\n\n    Returns: The prompt string that is sent to the LLM\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/chat/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chat","text":""},{"location":"reference/declarai/chat/#declarai.chat","title":"chat","text":"<p>Chat tasks are tasks that are meant to be used in an iterative fashion, where the user and the assistant exchange  messages.</p> <p>Unlike tasks, chat tasks are storing the message history in a <code>BaseChatMessageHistory</code> object, which is used to compile  the prompt sent to the LLM.</p> <p>At every iteration, the user message is added to the message history, and the prompt is compiled using the message  history. The prompt is then sent to the LLM, and the response is parsed and added to the message history.</p> <p>Classes:</p> Name Description <code>ChatMeta</code> <p>Metaclass for Chat classes. Used to enable the users to receive the chat instance when using the @chat decorator,</p> <code>Chat</code> <p>Chat class used for creating chat tasks.</p> <code>ChatDecorator</code> <p>A decorator class for receiving a chat class, fulfilled with the provided parameters, and returning a Chat object.</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat","title":"Chat","text":"<pre><code>Chat(\n    *,\n    operator: BaseChatOperator,\n    middlewares: List[TaskMiddleware] = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None\n)\n</code></pre> <p>             Bases: <code>BaseChat</code></p> <p>Chat class used for creating chat tasks.</p> <p>Chat tasks are tasks that are meant to be used in an iterative fashion, where the user and the assistant exchange messages.</p> <p>Attributes:</p> Name Type Description <code>is_declarai</code> <code>bool</code> <p>A class-level attribute indicating if the chat is of type 'declarai'. Always set to <code>True</code>.</p> <code>llm_response</code> <code>LLMResponse</code> <p>The response from the LLM (Language Model). This attribute is set during the execution of the chat.</p> <code>_kwargs</code> <code>Dict[str, Any]</code> <p>A dictionary to store additional keyword arguments, used for passing kwargs between the execution of the chat and the execution of the middlewares.</p> <code>middlewares</code> <code>List[TaskMiddleware] or None</code> <p>Middlewares used for every iteration of the chat.</p> <code>operator</code> <code>BaseChatOperator</code> <p>The operator used for the chat.</p> <code>conversation</code> <code>List[Message]</code> <p>Property that returns a list of messages exchanged in the chat. Keep in mind this list does not include the first system message. The system message is stored in the <code>system</code> attribute.</p> <code>_chat_history</code> <code>BaseChatMessageHistory</code> <p>The chat history mechanism for the chat.</p> <code>greeting</code> <code>str</code> <p>The greeting message for the chat.</p> <code>system</code> <code>str</code> <p>The system message for the chat.</p> <p>Parameters:</p> Name Type Description Default <code>operator</code> <code>BaseChatOperator</code> <p>The operator to use for the chat.</p> required <code>middlewares</code> <code>List[TaskMiddleware]</code> <p>Middlewares to use for every iteration of the chat. Defaults to</p> <code>None</code> <code>chat_history</code> <code>BaseChatMessageHistory</code> <p>Chat history mechanism to use. Defaults to <code>DEFAULT_CHAT_HISTORY()</code>.</p> <code>None</code> <code>greeting</code> <code>str</code> <p>Greeting message to use. Defaults to operator's greeting or None.</p> <code>None</code> <code>system</code> <code>str</code> <p>System message to use. Defaults to operator's system message or None.</p> <code>None</code> <p>Methods:</p> Name Description <code>_exec_middlewares</code> <p>Execute the task middlewares and the task itself</p> <code>compile</code> <p>Compiles a list of messages to be sent to the LLM by the operator.</p> <code>add_message</code> <p>Interface to add a message to the chat history.</p> <code>_exec</code> <p>Executes the call to the LLM.</p> <code>_exec_with_message_state</code> <p>Executes the call to the LLM and adds the response to the chat history as an assistant message.</p> <code>__call__</code> <p>Executes the call to the LLM, based on the messages passed as argument, and the llm_params.</p> <code>send</code> <p>Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns</p> <p>Attributes:</p> Name Type Description <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p> <code>conversation</code> <code>List[Message]</code> <p>Returns:</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __init__(\n    self,\n    *,\n    operator: BaseChatOperator,\n    middlewares: List[TaskMiddleware] = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n):\n    self.middlewares = middlewares\n    self.operator = operator\n    self._chat_history = chat_history or DEFAULT_CHAT_HISTORY()\n    self.greeting = greeting or self.operator.greeting\n    self.system = system or self.operator.system\n    self.__set_memory()\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.conversation","title":"conversation  <code>property</code>","text":"<pre><code>conversation: List[Message]\n</code></pre> <p>Returns:</p> Type Description <code>List[Message]</code> <p>a list of messages exchanged in the chat. Keep in mind this list does not include the first system message.</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed.</p>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    messages: List[Message],\n    llm_params: LLMParamsType = None,\n    **kwargs: LLMParamsType\n) -&gt; Any\n</code></pre> <p>Executes the call to the LLM, based on the messages passed as argument, and the llm_params. The llm_params are passed as a dictionary, and they are used to override the default llm_params of the operator. The llm_params also have priority over the params that were used to initialize the chat within the decorator.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to pass to the LLM.</p> required <code>llm_params</code> <code>LLMParamsType</code> <p>The llm_params to use for the call to the LLM.</p> <code>None</code> <code>**kwargs</code> <p>run time kwargs to use when formatting the system message prompt.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response from the LLM.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __call__(\n    self, *, messages: List[Message], llm_params: LLMParamsType = None, **kwargs\n) -&gt; Any:\n\"\"\"\n    Executes the call to the LLM, based on the messages passed as argument, and the llm_params.\n    The llm_params are passed as a dictionary, and they are used to override the default llm_params of the operator.\n    The llm_params also have priority over the params that were used to initialize the chat within the decorator.\n    Args:\n        messages: The messages to pass to the LLM.\n        llm_params: The llm_params to use for the call to the LLM.\n        **kwargs: run time kwargs to use when formatting the system message prompt.\n\n    Returns:\n        The parsed response from the LLM.\n\n    \"\"\"\n    kwargs[\"messages\"] = messages\n    runtime_llm_params = (\n        llm_params or self.llm_params\n    )  # order is important! We prioritize runtime params that\n    if runtime_llm_params:\n        kwargs[\"llm_params\"] = runtime_llm_params\n    return self._exec_with_message_state(kwargs)\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat._exec","title":"_exec","text":"<pre><code>_exec(kwargs) -&gt; LLMResponse\n</code></pre> <p>Executes the call to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to pass to the LLM like <code>temperature</code>, <code>max_tokens</code>, etc.</p> required <p>Returns:</p> Type Description <code>LLMResponse</code> <p>The raw response from the LLM, together with the metadata.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def _exec(self, kwargs) -&gt; LLMResponse:\n\"\"\"\n    Executes the call to the LLM.\n\n    Args:\n        kwargs: Keyword arguments to pass to the LLM like `temperature`, `max_tokens`, etc.\n\n    Returns:\n         The raw response from the LLM, together with the metadata.\n    \"\"\"\n    self.llm_response = self.operator.predict(**kwargs)\n    return self.llm_response\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat._exec_middlewares","title":"_exec_middlewares  <code>abstractmethod</code>","text":"<pre><code>_exec_middlewares(kwargs) -&gt; Any\n</code></pre> <p>Execute the task middlewares and the task itself</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>the runtime keyword arguments that are used to compile the task prompt.</p> required Source code in <code>src/declarai/_base.py</code> <pre><code>@abstractmethod\ndef _exec_middlewares(self, kwargs) -&gt; Any:\n\"\"\"\n    Execute the task middlewares and the task itself\n    Args:\n        kwargs: the runtime keyword arguments that are used to compile the task prompt.\n\n    Returns: The result of the task, which is the result of the operator. Same as `_exec`.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat._exec_with_message_state","title":"_exec_with_message_state","text":"<pre><code>_exec_with_message_state(kwargs) -&gt; Any\n</code></pre> <p>Executes the call to the LLM and adds the response to the chat history as an assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Keyword arguments to pass to the LLM like <code>temperature</code>, <code>max_tokens</code>, etc.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The parsed response from the LLM.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def _exec_with_message_state(self, kwargs) -&gt; Any:\n\"\"\"\n    Executes the call to the LLM and adds the response to the chat history as an assistant message.\n    Args:\n        kwargs: Keyword arguments to pass to the LLM like `temperature`, `max_tokens`, etc.\n\n    Returns:\n        The parsed response from the LLM.\n    \"\"\"\n    raw_response = self._exec(kwargs).response\n    self.add_message(raw_response, role=MessageRole.assistant)\n    if self.operator.parsed_send_func:\n        return self.operator.parsed_send_func.parse(raw_response)\n    return raw_response\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.add_message","title":"add_message","text":"<pre><code>add_message(message: str, role: MessageRole) -&gt; None\n</code></pre> <p>Interface to add a message to the chat history.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to add to the chat history.</p> required <code>role</code> <code>MessageRole</code> <p>The role of the message (assistant, user, system, etc.)</p> required Source code in <code>src/declarai/chat.py</code> <pre><code>def add_message(self, message: str, role: MessageRole) -&gt; None:\n\"\"\"\n    Interface to add a message to the chat history.\n    Args:\n        message (str): The message to add to the chat history.\n        role (MessageRole): The role of the message (assistant, user, system, etc.)\n    \"\"\"\n    self._chat_history.add_message(Message(message=message, role=role))\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; List[Message]\n</code></pre> <p>Compiles a list of messages to be sent to the LLM by the operator. This is done by accessing the ._chat_history.history attribute. The kwargs that are passed to the compile method are onlu used to populate the system message prompt.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>System message prompt kwargs.</p> <code>{}</code> Source code in <code>src/declarai/chat.py</code> <pre><code>def compile(self, **kwargs) -&gt; List[Message]:\n\"\"\"\n    Compiles a list of messages to be sent to the LLM by the operator.\n    This is done by accessing the ._chat_history.history attribute.\n    The kwargs that are passed to the compile method are onlu used to populate the system message prompt.\n    Args:\n        **kwargs: System message prompt kwargs.\n\n    Returns: List[Message] - The compiled messages that will be sent to the LLM.\n\n    \"\"\"\n    compiled = self.operator.compile(messages=self._chat_history.history, **kwargs)\n    return compiled\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.Chat.send","title":"send","text":"<pre><code>send(\n    message: str,\n    llm_params: Union[LLMParamsType, Dict[str, Any]] = None,\n    **kwargs: Union[LLMParamsType, Dict[str, Any]]\n) -&gt; Any\n</code></pre> <p>Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns  the raw response from the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> required <code>llm_params</code> <code>Union[LLMParamsType, Dict[str, Any]]</code> <code>None</code> <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Final response from the LLM, after parsing.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def send(\n    self,\n    message: str,\n    llm_params: Union[LLMParamsType, Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Any:\n\"\"\"\n    Interface that allows the user to send a message to the LLM. It takes a raw string as input, and returns\n     the raw response from the LLM.\n    Args:\n        message:\n        llm_params:\n        **kwargs:\n\n    Returns:\n        Final response from the LLM, after parsing.\n\n    \"\"\"\n    self.add_message(message, role=MessageRole.user)\n    return self(\n        messages=self._chat_history.history, llm_params=llm_params, **kwargs\n    )\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatDecorator","title":"ChatDecorator","text":"<pre><code>ChatDecorator(\n    llm_settings: LLMSettings, **kwargs: LLMSettings\n)\n</code></pre> <p>A decorator class for receiving a chat class, fulfilled with the provided parameters, and returning a Chat object.</p> <p>This class provides the <code>chat</code> method which acts as a decorator to create a Chat object.</p> <p>Parameters:</p> Name Type Description Default <code>llm_settings</code> <code>LLMSettings</code> <p>Settings for the LLM like model provider, model name, etc.</p> required <code>**kwargs</code> <p>Additional keyword arguments like openai_api_key, etc.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>llm_settings</code> <code>LLMSettings</code> <p>Settings for the LLM.</p> <code>_kwargs</code> <code>dict</code> <p>Dictionary storing keyword arguments passed during initialization.</p> <p>Methods:</p> Name Description <code>chat</code> <p>Decorator method that converts a class into a chat task class.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __init__(self, llm_settings: LLMSettings, **kwargs):\n    self.llm_settings = llm_settings\n    self._kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatDecorator.chat","title":"chat","text":"<pre><code>chat(\n    cls: Type = None,\n    *,\n    middlewares: List[TaskMiddleware] = None,\n    llm_params: LLMParamsType = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None\n)\n</code></pre> <p>Decorator method that converts a class into a chat task class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The original class that is being decorated.</p> <code>None</code> <code>middlewares</code> <code>List[TaskMiddleware]</code> <p>Middlewares to use for every iteration of the chat. Defaults to None.</p> <code>None</code> <code>llm_params</code> <code>LLMParamsType</code> <p>Parameters for the LLM. Defaults to None.</p> <code>None</code> <code>chat_history</code> <code>BaseChatMessageHistory</code> <p>Chat history mechanism to use. Defaults to None.</p> <code>None</code> <code>greeting</code> <code>str</code> <p>Greeting message to use. Defaults to None.</p> <code>None</code> <code>system</code> <code>str</code> <p>System message to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[Chat]</code> <p>A new Chat class that inherits from the original class and has chat capabilities.</p> Example <pre><code> @ChatDecorator.chat(llm_params={\"temperature\": 0.5})\n class MyChat:\n    ...\n\n @ChatDecorator.chat\n class MyChat:\n    ...\n</code></pre> Source code in <code>src/declarai/chat.py</code> <pre><code>def chat(\n    self,\n    cls: Type = None,\n    *,\n    middlewares: List[TaskMiddleware] = None,\n    llm_params: LLMParamsType = None,\n    chat_history: BaseChatMessageHistory = None,\n    greeting: str = None,\n    system: str = None,\n):\n\"\"\"\n    Decorator method that converts a class into a chat task class.\n\n    Args:\n        cls (Type, optional): The original class that is being decorated.\n        middlewares (List[TaskMiddleware], optional): Middlewares to use for every iteration of the chat.\n         Defaults to None.\n        llm_params (LLMParamsType, optional): Parameters for the LLM. Defaults to None.\n        chat_history (BaseChatMessageHistory, optional): Chat history mechanism to use. Defaults to None.\n        greeting (str, optional): Greeting message to use. Defaults to None.\n        system (str, optional): System message to use. Defaults to None.\n\n    Returns:\n        (Type[Chat]): A new Chat class that inherits from the original class and has chat capabilities.\n\n\n    Example:\n        ```python\n         @ChatDecorator.chat(llm_params={\"temperature\": 0.5})\n         class MyChat:\n            ...\n\n         @ChatDecorator.chat\n         class MyChat:\n            ...\n        ```\n\n    \"\"\"\n    operator_type, llm = resolve_operator(\n        self.llm_settings, operator_type=\"chat\", **self._kwargs\n    )\n\n    def wrap(cls) -&gt; Type[Chat]:\n        non_private_methods = {\n            method_name: method\n            for method_name, method in cls.__dict__.items()\n            if not method_name.startswith(\"__\") and callable(method)\n        }\n        if \"send\" in non_private_methods:\n            non_private_methods.pop(\"send\")\n\n        parsed_cls = PythonParser(cls)\n\n        _decorator_kwargs = dict(\n            operator=operator_type(\n                llm=llm,\n                system=parsed_cls.docstring_freeform,\n                parsed=parsed_cls,\n                llm_params=llm_params,\n            ),\n            middlewares=middlewares,\n            chat_history=chat_history,\n            greeting=greeting,\n            system=system,\n        )\n\n        new_chat: Type[Chat] = type(cls.__name__, (Chat,), {})  # noqa\n        new_chat.__name__ = cls.__name__\n        new_chat._init_args = ()  # any positional arguments\n        new_chat._init_kwargs = _decorator_kwargs\n        for method_name, method in non_private_methods.items():\n            if isinstance(method, Task):\n                _method = method\n            else:\n                _method = partial(method, new_chat)\n            setattr(new_chat, method_name, _method)\n        return new_chat\n\n    if cls is None:\n        return wrap\n    return wrap(cls)\n</code></pre>"},{"location":"reference/declarai/chat/#declarai.chat.ChatMeta","title":"ChatMeta","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for Chat classes. Used to enable the users to receive the chat instance when using the @chat decorator, and still be able to \"instantiate\" the class.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are</p>"},{"location":"reference/declarai/chat/#declarai.chat.ChatMeta.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are the same as the ones used for the decorator, but the ones used for the class initialization are precedence.</p> Source code in <code>src/declarai/chat.py</code> <pre><code>def __call__(cls, *args, **kwargs):\n\"\"\"\n    Initialize the Chat instance for the second time, after the decorator has been applied. The parameters are\n    the same as the ones used for the decorator, but the ones used for the class initialization are precedence.\n    Returns: Chat instance\n\n    \"\"\"\n    # Determine which arguments to use for initialization\n    final_args = args if args else cls._init_args\n    final_kwargs = {**cls._init_kwargs, **kwargs}\n\n    # Create and initialize the instance\n    instance = super().__call__(*final_args, **final_kwargs)\n\n    # Always set the __name__ attribute on the instance\n    instance.__name__ = cls.__name__\n\n    return instance\n</code></pre>"},{"location":"reference/declarai/declarai/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> declarai","text":""},{"location":"reference/declarai/declarai/#declarai.declarai","title":"declarai","text":"<p>This is the main entry point for declarai. It is used to decorate the package functionalities and serve as the main interface for the user.</p> <p>Classes:</p> Name Description <code>Declarai</code> <p>A root interface to declarai.</p> <p>Functions:</p> Name Description <code>magic</code> <p>This is an empty method used as a potential replacement for using the docstring for passing</p>"},{"location":"reference/declarai/declarai/#declarai.declarai.Declarai","title":"Declarai","text":"<pre><code>Declarai(**kwargs)\n</code></pre> <p>A root interface to declarai. This class allows creating tasks and other declarai provided tools.</p> <p>There are overloads for the constructor that allow for a more declarative way of creating tasks. based on the provider and model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>A set of keyword arguments that are passed to the LLMSettings class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>llm_settings</code> <code>LLMSettings</code> <p>A settings object that is used to configure the LLM.</p> <code>task</code> <code>Callable</code> <p>A decorator that is used to create tasks.</p> <code>experimental</code> <code>Any</code> <p>A namespace for experimental features.</p> <code>experimental.chat</code> <code>Callable</code> <p>A decorator that is used to create chat operators.</p> Source code in <code>src/declarai/declarai.py</code> <pre><code>def __init__(self, **kwargs):\n    self.llm_settings = LLMSettings(**kwargs)\n    self.task = TaskDecorator(self.llm_settings).task\n\n    class Experimental:\n        chat = ChatDecorator(self.llm_settings).chat\n\n    self.experimental = Experimental\n</code></pre>"},{"location":"reference/declarai/declarai/#declarai.declarai.magic","title":"magic","text":"<pre><code>magic(\n    return_name: Optional[str] = None,\n    *,\n    task_desc: Optional[str] = None,\n    input_desc: Optional[Dict[str, str]] = None,\n    output_desc: Optional[str] = None,\n    **kwargs: Optional[str]\n) -&gt; Any\n</code></pre> <p>This is an empty method used as a potential replacement for using the docstring for passing parameters to the LLM builder. It can also serve as a fake use of arguments in the defined functions as to simplify handling of lint rules for llms functions.</p> Example <pre><code>@declarai.task\ndef add(a: int, b: int) -&gt; int:\n    return magic(a, b)\n</code></pre> Source code in <code>src/declarai/declarai.py</code> <pre><code>def magic(\n    return_name: Optional[str] = None,\n    *,\n    task_desc: Optional[str] = None,\n    input_desc: Optional[Dict[str, str]] = None,\n    output_desc: Optional[str] = None,\n    **kwargs\n) -&gt; Any:\n\"\"\"\n    This is an empty method used as a potential replacement for using the docstring for passing\n    parameters to the LLM builder. It can also serve as a fake use of arguments in the defined\n    functions as to simplify handling of lint rules for llms functions.\n\n    Example:\n        ```py\n        @declarai.task\n        def add(a: int, b: int) -&gt; int:\n            return magic(a, b)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/declarai/task/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> task","text":""},{"location":"reference/declarai/task/#declarai.task","title":"task","text":"<p>Task</p> <p>Provides the most basic component to interact with an LLM. LLMs are often interacted with via an API. In order to provide prompts and receive predictions, we will need to create the following: - parse the provided python code - Translate the parsed data into the proper prompt for the LLM - Send the request to the LLM and parse the output back into python</p> <p>This class is an orchestrator that calls a parser and operators to perform the above tasks. while the parser is meant to be shared across cases, as python code has a consistent interface, the different LLM API providers as well as custom models have different APIs with different expected prompt structures. For that reason, there are multiple implementations of operators, depending on the required use case.</p> <p>Classes:</p> Name Description <code>FutureTask</code> <p>A FutureTask is a wrapper around the task that is returned from the <code>plan</code> method.</p> <code>Task</code> <p>Initializes the Task</p> <code>TaskDecorator</code> <p>The TaskDecorator is used to create a task. It is used as a decorator on a function that will be used as a task.</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask","title":"FutureTask","text":"<pre><code>FutureTask(\n    exec_func: Callable[[], Any],\n    kwargs: Dict[str, Any],\n    compiled_template: str,\n    populated_prompt: str,\n)\n</code></pre> <p>A FutureTask is a wrapper around the task that is returned from the <code>plan</code> method. It used to create a lazy execution of the task, and to provide additional information about the task. The only functionality that is provided by the FutureTask is the <code>__call__</code> method, which executes the task.</p> <p>Parameters:</p> Name Type Description Default <code>exec_func</code> <code>Callable[[], Any]</code> <p>the function to execute when the future task is called</p> required <code>kwargs</code> <code>Dict[str, Any]</code> <p>the kwargs that were passed to the task</p> required <code>compiled_template</code> <code>str</code> <p>the compiled template that was populated by the task</p> required <code>populated_prompt</code> <code>str</code> <p>the populated prompt that was populated by the task</p> required <p>Methods:</p> Name Description <code>__call__</code> <p>executes the task</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Calls the <code>exec_func</code> attribute of the FutureTask</p> <p>Attributes:</p> Name Type Description <code>populated_prompt</code> <code>str</code> <p>Returns the populated prompt that was populated by the task</p> <code>compiled_template</code> <code>str</code> <p>Returns the compiled template that was populated by the task</p> <code>task_kwargs</code> <code>Dict[str, Any]</code> <p>Returns the kwargs that were passed to the task</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(\n    self,\n    exec_func: Callable[[], Any],\n    kwargs: Dict[str, Any],\n    compiled_template: str,\n    populated_prompt: str,\n):\n    self.exec_func = exec_func\n    self.__populated_prompt = populated_prompt\n    self.__compiled_template = compiled_template\n    self.__kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.compiled_template","title":"compiled_template  <code>property</code>","text":"<pre><code>compiled_template: str\n</code></pre> <p>Returns the compiled template that was populated by the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.populated_prompt","title":"populated_prompt  <code>property</code>","text":"<pre><code>populated_prompt: str\n</code></pre> <p>Returns the populated prompt that was populated by the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.task_kwargs","title":"task_kwargs  <code>property</code>","text":"<pre><code>task_kwargs: Dict[str, Any]\n</code></pre> <p>Returns the kwargs that were passed to the task</p>"},{"location":"reference/declarai/task/#declarai.task.FutureTask.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Calls the <code>exec_func</code> attribute of the FutureTask</p> <p>Returns:</p> Type Description <code>Any</code> <p>the response from the <code>exec_func</code></p> Source code in <code>src/declarai/task.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Calls the `exec_func` attribute of the FutureTask\n    Returns:\n        the response from the `exec_func`\n    \"\"\"\n    return self.exec_func()\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task","title":"Task","text":"<pre><code>Task(\n    operator: BaseOperator,\n    middlewares: List[TaskMiddleware] = None,\n)\n</code></pre> <p>             Bases: <code>BaseTask</code></p> <p>Initializes the Task</p> <p>Parameters:</p> Name Type Description Default <code>operator</code> <code>BaseOperator</code> <p>the operator to use to interact with the LLM</p> required <code>middlewares</code> <code>List[TaskMiddleware]</code> <p>the middlewares to use while executing the task</p> <code>None</code> <code>**kwargs</code> required <p>Attributes:</p> Name Type Description <code>operator</code> <p>the operator to use to interact with the LLM</p> <code>llm_response</code> <code>LLMResponse</code> <p>the response from the LLM</p> <code>_kwargs</code> <code>Dict[str, Any]</code> <p>the kwargs that were passed to the task are set as attributes on the task and passed to the middlewares</p> <p>Methods:</p> Name Description <code>compile</code> <p>Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM.</p> <code>plan</code> <p>Populates the compiled template with the actual data.</p> <code>__call__</code> <p>Orchestrates the execution of the task.</p> <p>Attributes:</p> Name Type Description <code>llm_params</code> <code>LLMParamsType</code> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(\n    self, operator: BaseOperator, middlewares: List[TaskMiddleware] = None\n):\n    self.middlewares = middlewares\n    self.operator = operator\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.llm_params","title":"llm_params  <code>property</code>","text":"<pre><code>llm_params: LLMParamsType\n</code></pre> <p>Return the LLM parameters that are saved on the operator. These parameters are sent to the LLM when the task is executed.</p>"},{"location":"reference/declarai/task/#declarai.task.Task.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    llm_params: LLMParamsType = None,\n    **kwargs: LLMParamsType\n) -&gt; Any\n</code></pre> <p>Orchestrates the execution of the task.</p> <p>Parameters:</p> Name Type Description Default <code>llm_params</code> <code>LLMParamsType</code> <p>the params to pass to the LLM. If provided, they will override the params that were passed during initialization</p> <code>None</code> <code>**kwargs</code> <p>kwargs that are used to compile the template and populate the prompt.</p> <code>{}</code> Source code in <code>src/declarai/task.py</code> <pre><code>def __call__(self, *, llm_params: LLMParamsType = None, **kwargs) -&gt; Any:\n\"\"\"\n    Orchestrates the execution of the task.\n    Args:\n        llm_params: the params to pass to the LLM. If provided, they will override the params that were passed during initialization\n        **kwargs: kwargs that are used to compile the template and populate the prompt.\n\n    Returns: the user defined return type of the task\n\n    \"\"\"\n    self._kwargs = kwargs\n    runtime_llm_params = (\n        llm_params or self.llm_params\n    )  # order is important! We prioritize runtime params that\n    # were passed\n    if runtime_llm_params:\n        self._kwargs[\"llm_params\"] = runtime_llm_params\n    return self._exec_middlewares(kwargs)\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; Any\n</code></pre> <p>Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM. Can be used for debugging purposes as well, to see what the prompt will look like before sending it to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>the data to populate the template with</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>the compiled template</p> Source code in <code>src/declarai/task.py</code> <pre><code>def compile(self, **kwargs) -&gt; Any:\n\"\"\"\n    Compiles the prompt to be sent to the LLM. This is the first step in the process of interacting with the LLM.\n    Can be used for debugging purposes as well, to see what the prompt will look like before sending it to the LLM.\n    Args:\n        **kwargs: the data to populate the template with\n\n    Returns:\n         the compiled template\n\n    \"\"\"\n    return self.operator.compile(**kwargs)\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.Task.plan","title":"plan","text":"<pre><code>plan(**kwargs) -&gt; FutureTask\n</code></pre> <p>Populates the compiled template with the actual data.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>the data to populate the template with</p> <code>{}</code> <p>Returns:</p> Type Description <code>FutureTask</code> <p>a FutureTask that can be used to execute the task in a lazy manner</p> Source code in <code>src/declarai/task.py</code> <pre><code>def plan(self, **kwargs) -&gt; FutureTask:\n\"\"\"\n    Populates the compiled template with the actual data.\n    Args:\n        **kwargs: the data to populate the template with\n    Returns:\n         a FutureTask that can be used to execute the task in a lazy manner\n    \"\"\"\n\n    populated_prompt = self.compile(**kwargs)\n    return FutureTask(\n        self.__call__,\n        kwargs=kwargs,\n        compiled_template=self.compile(),\n        populated_prompt=populated_prompt,\n    )\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.TaskDecorator","title":"TaskDecorator","text":"<pre><code>TaskDecorator(\n    llm_settings: LLMSettings, **kwargs: LLMSettings\n)\n</code></pre> <p>The TaskDecorator is used to create a task. It is used as a decorator on a function that will be used as a task.</p> <p>Parameters:</p> Name Type Description Default <code>llm_settings</code> <code>LLMSettings</code> <p>the settings that define which LLM to use</p> required <code>**kwargs</code> <p>additional llm_settings like open_ai_api_key etc.</p> <code>{}</code> <p>Methods:</p> Name Description <code>task</code> <p>the decorator that creates the task</p> <p>Methods:</p> Name Description <code>task</code> <p>The decorator that creates the task</p> Source code in <code>src/declarai/task.py</code> <pre><code>def __init__(self, llm_settings: LLMSettings, **kwargs):\n    self.llm_settings = llm_settings\n    self._kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/task/#declarai.task.TaskDecorator.task","title":"task","text":"<pre><code>task(\n    func: Optional[Callable] = None,\n    *,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    llm_params: LLMParamsType = None\n)\n</code></pre> <p>The decorator that creates the task</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Optional[Callable]</code> <p>the function to decorate that represents the task</p> <code>None</code> <code>middlewares</code> <code>List[Type[TaskMiddleware]]</code> <p>middleware to use while executing the task</p> <code>None</code> <code>llm_params</code> <code>LLMParamsType</code> <p>llm_params to use when calling the llm</p> <code>None</code> <p>Returns:</p> Type Description <code>Task</code> <p>the task that was created</p> Source code in <code>src/declarai/task.py</code> <pre><code>def task(\n    self,\n    func: Optional[Callable] = None,\n    *,\n    middlewares: List[Type[TaskMiddleware]] = None,\n    llm_params: LLMParamsType = None,\n):\n\"\"\"\n    The decorator that creates the task\n    Args:\n        func: the function to decorate that represents the task\n        middlewares: middleware to use while executing the task\n        llm_params: llm_params to use when calling the llm\n\n    Returns:\n        (Task): the task that was created\n\n    \"\"\"\n    operator_type, llm = resolve_operator(\n        self.llm_settings, operator_type=\"task\", **self._kwargs\n    )\n\n    def wrap(_func: Callable) -&gt; Task:\n        operator = operator_type(\n            parsed=PythonParser(_func),\n            llm=llm,\n            llm_params=llm_params,\n        )\n        llm_task = Task(operator=operator, middlewares=middlewares)\n        llm_task.__name__ = _func.__name__\n        return llm_task\n\n    if func is None:\n        return wrap\n\n    return wrap(func)\n</code></pre>"},{"location":"reference/declarai/core/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core","text":""},{"location":"reference/declarai/core/#declarai.core","title":"core","text":"<p>Modules:</p> Name Description <code>core_settings</code> <p>This module contains the core settings for the declarai project.</p>"},{"location":"reference/declarai/core/core_settings/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> core_settings","text":""},{"location":"reference/declarai/core/core_settings/#declarai.core.core_settings","title":"core_settings","text":"<p>This module contains the core settings for the declarai project. In order to create proper separation from existing code on the client's environment, we require all environment variables used by <code>declarai</code> be prefixed with <code>DECLARAI_</code>. This way we do not interfere with any existing environment variables.</p>"},{"location":"reference/declarai/memory/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> memory","text":""},{"location":"reference/declarai/memory/#declarai.memory","title":"memory","text":"<p>Modules:</p> Name Description <code>mongodb</code> <p>This module contains the MongoDBMessageHistory class, which is used to store chat message history in a MongoDB database.</p> <code>postgres</code> <p>This module contains the PostgresMessageHistory class, which is used to store chat message history in a PostgreSQL database.</p> <code>in_memory</code> <p>This module contains the in-memory implementation of the chat message history.</p> <code>file</code> <p>This module contains the FileMessageHistory class, which is used to store chat message history in a local file.</p> <code>base</code> <p>Base class for the memory module.</p> <code>redis</code> <p>This module contains the RedisMessageHistory class, which is used to store chat message history in a Redis database.</p>"},{"location":"reference/declarai/memory/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"reference/declarai/memory/base/#declarai.memory.base","title":"base","text":"<p>Base class for the memory module.</p> <p>Classes:</p> Name Description <code>BaseChatMessageHistory</code> <p>Abstract class to store the chat message history.</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory","title":"BaseChatMessageHistory","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class to store the chat message history.</p> <p>See <code>ChatMessageHistory</code> for default implementation.</p> <p>Methods:</p> Name Description <code>add_message</code> <p>Add a Message object to the state.</p> <code>clear</code> <p>Remove all messages from the state</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Return the chat message history</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.history","title":"history  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Return the chat message history</p> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List of Message objects</p>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.add_message","title":"add_message  <code>abstractmethod</code>","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Add a Message object to the state.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message object to add to the state</p> required Source code in <code>src/declarai/memory/base.py</code> <pre><code>@abstractmethod\ndef add_message(self, message: Message) -&gt; None:\n\"\"\"\n    Add a Message object to the state.\n\n    Args:\n        message: Message object to add to the state\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/memory/base/#declarai.memory.base.BaseChatMessageHistory.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Remove all messages from the state</p> Source code in <code>src/declarai/memory/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n\"\"\"\n    Remove all messages from the state\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/memory/file/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> file","text":""},{"location":"reference/declarai/memory/file/#declarai.memory.file","title":"file","text":"<p>This module contains the FileMessageHistory class, which is used to store chat message history in a local file.</p> <p>Classes:</p> Name Description <code>FileMessageHistory</code> <p>Chat message history that stores history in a local file.</p>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory","title":"FileMessageHistory","text":"<pre><code>FileMessageHistory(file_path: Optional[str] = None)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Optional[str]</code> <p>path of the local file to store the messages. if not passed the messages will be stored in a temporary file, and a warning will be logged.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in the local file</p> <code>clear</code> <p>Clear session memory from the local file</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def __init__(self, file_path: Optional[str] = None):\n    super().__init__()\n    if not file_path:\n        # Create a temporary file and immediately close it to get its name.\n        temp = tempfile.NamedTemporaryFile(delete=False)\n        self.file_path = Path(temp.name)\n        self.file_path.write_text(json.dumps([]))\n        logger.warning(\n            \"No file path provided to store the messages. \"\n            f\"Messages will be stored in a temporary file path: {self.file_path}\"\n        )\n    else:\n        self.file_path = Path(file_path)\n\n    if not self.file_path.exists():\n        self.file_path.touch()\n        self.file_path.write_text(json.dumps([]))\n</code></pre>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from the local file</p>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in the local file\"\"\"\n    messages = self.history.copy()\n    messages.append(message)\n    messages_dict = [msg.dict() for msg in messages]\n    self.file_path.write_text(json.dumps(messages_dict))\n</code></pre>"},{"location":"reference/declarai/memory/file/#declarai.memory.file.FileMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from the local file</p> Source code in <code>src/declarai/memory/file.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from the local file\"\"\"\n    self.file_path.write_text(json.dumps([]))\n</code></pre>"},{"location":"reference/declarai/memory/in_memory/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> in_memory","text":""},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory","title":"in_memory","text":"<p>This module contains the in-memory implementation of the chat message history.</p> <p>Classes:</p> Name Description <code>InMemoryMessageHistory</code> <p>This memory implementation stores all messages in memory in a list.</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory","title":"InMemoryMessageHistory","text":"<p>             Bases: <code>BaseChatMessageHistory</code>, <code>BaseModel</code></p> <p>This memory implementation stores all messages in memory in a list.</p> <p>Methods:</p> Name Description <code>add_message</code> <p>Adds a message to the list of messages stored in memory.</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Returns the list of messages stored in memory.</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Returns the list of messages stored in memory. :return: List of messages</p>"},{"location":"reference/declarai/memory/in_memory/#declarai.memory.in_memory.InMemoryMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Adds a message to the list of messages stored in memory. :param message: the message content and role</p> Source code in <code>src/declarai/memory/in_memory.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"\n    Adds a message to the list of messages stored in memory.\n    :param message: the message content and role\n    \"\"\"\n    self.messages.append(message)\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> mongodb","text":""},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb","title":"mongodb","text":"<p>This module contains the MongoDBMessageHistory class, which is used to store chat message history in a MongoDB database.</p> <p>Classes:</p> Name Description <code>MongoDBMessageHistory</code> <p>Chat message history that stores history in MongoDB.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_DBNAME</code> <p>A database name for the MongoDB database.</p> <code>DEFAULT_COLLECTION_NAME</code> <p>A collection name for the MongoDB database.</p> <code>DEFAULT_CONNECTION_STRING</code> <p>A connection string for a MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_COLLECTION_NAME","title":"DEFAULT_COLLECTION_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_COLLECTION_NAME = 'message_store'\n</code></pre> <p>A collection name for the MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_CONNECTION_STRING","title":"DEFAULT_CONNECTION_STRING  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONNECTION_STRING = 'mongodb://localhost:27017'\n</code></pre> <p>A connection string for a MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.DEFAULT_DBNAME","title":"DEFAULT_DBNAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DBNAME = 'chat_history'\n</code></pre> <p>A database name for the MongoDB database.</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory","title":"MongoDBMessageHistory","text":"<pre><code>MongoDBMessageHistory(\n    session_id: str,\n    connection_string: str = DEFAULT_CONNECTION_STRING,\n    database_name: str = DEFAULT_DBNAME,\n    collection_name: str = DEFAULT_COLLECTION_NAME,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in MongoDB.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>connection string to connect to MongoDB</p> <code>DEFAULT_CONNECTION_STRING</code> <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>database_name</code> <code>str</code> <p>name of the database to use</p> <code>DEFAULT_DBNAME</code> <code>collection_name</code> <code>str</code> <p>name of the collection to use</p> <code>DEFAULT_COLLECTION_NAME</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in MongoDB</p> <code>clear</code> <p>Clear session memory from MongoDB</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    connection_string: str = DEFAULT_CONNECTION_STRING,\n    database_name: str = DEFAULT_DBNAME,\n    collection_name: str = DEFAULT_COLLECTION_NAME,\n):\n    try:\n        from pymongo import MongoClient\n    except ImportError:\n        raise ImportError(\n            \"Could not import pymongo python package. \"\n            \"Please install it with `pip install pymongo`.\"\n        )\n\n    self.connection_string = connection_string\n    self.session_id = session_id\n    self.database_name = database_name\n    self.collection_name = collection_name\n\n    self.client: MongoClient = MongoClient(connection_string)\n    self.db = self.client[database_name]\n    self.collection = self.db[collection_name]\n    self.collection.create_index(\"SessionId\")\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from MongoDB</p>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in MongoDB\"\"\"\n    self.collection.insert_one(\n        {\n            \"SessionId\": self.session_id,\n            \"History\": json.dumps(message.dict()),\n        }\n    )\n</code></pre>"},{"location":"reference/declarai/memory/mongodb/#declarai.memory.mongodb.MongoDBMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from MongoDB</p> Source code in <code>src/declarai/memory/mongodb.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from MongoDB\"\"\"\n    self.collection.delete_many({\"SessionId\": self.session_id})\n</code></pre>"},{"location":"reference/declarai/memory/postgres/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> postgres","text":""},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres","title":"postgres","text":"<p>This module contains the PostgresMessageHistory class, which is used to store chat message history in a PostgreSQL database.</p> <p>Classes:</p> Name Description <code>PostgresMessageHistory</code> <p>Chat message history that stores history in a PostgreSQL database.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_TABLE_NAME</code> <p>A table name for the PostgreSQL database.</p> <code>DEFAULT_CONNECTION_STRING</code> <p>A connection string for a PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.DEFAULT_CONNECTION_STRING","title":"DEFAULT_CONNECTION_STRING  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CONNECTION_STRING = (\n    \"postgresql://postgres:postgres@localhost:5432/postgres\"\n)\n</code></pre> <p>A connection string for a PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.DEFAULT_TABLE_NAME","title":"DEFAULT_TABLE_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TABLE_NAME = 'message_store'\n</code></pre> <p>A table name for the PostgreSQL database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory","title":"PostgresMessageHistory","text":"<pre><code>PostgresMessageHistory(\n    session_id: str,\n    connection_string: Optional[\n        str\n    ] = DEFAULT_CONNECTION_STRING,\n    table_name: str = DEFAULT_TABLE_NAME,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a PostgreSQL database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>connection_string</code> <code>Optional[str]</code> <p>Database connection string.</p> <code>DEFAULT_CONNECTION_STRING</code> <code>table_name</code> <code>str</code> <p>Name of the table to use.</p> <code>DEFAULT_TABLE_NAME</code> <p>Methods:</p> Name Description <code>_initialize_tables</code> <p>Initialize the tables if they don't exist.</p> <code>add_message</code> <p>Add a message to the database.</p> <code>clear</code> <p>Clear session memory from the database.</p> <code>close</code> <p>Close cursor and connection.</p> <code>__del__</code> <p>Destructor to close cursor and connection.</p> <p>Attributes:</p> Name Type Description <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    connection_string: Optional[str] = DEFAULT_CONNECTION_STRING,\n    table_name: str = DEFAULT_TABLE_NAME,\n):\n    try:\n        import psycopg2  # pylint: disable=import-outside-toplevel\n    except ImportError:\n        raise ImportError(\n            \"Cannot import psycopg2.\"\n            \"Please install psycopg2 to use PostgresMessageHistory.\"\n        )\n    self.conn = psycopg2.connect(connection_string)\n    self.cursor = self.conn.cursor()\n    self.table_name = table_name\n    self.session_id = session_id\n    self._initialize_tables()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from the database.</p>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Destructor to close cursor and connection.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def __del__(self):\n\"\"\"Destructor to close cursor and connection.\"\"\"\n    if hasattr(self, \"cursor\"):\n        self.cursor.close()\n    if hasattr(self, \"conn\"):\n        self.conn.close()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory._initialize_tables","title":"_initialize_tables","text":"<pre><code>_initialize_tables()\n</code></pre> <p>Initialize the tables if they don't exist.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def _initialize_tables(self):\n\"\"\"Initialize the tables if they don't exist.\"\"\"\n    create_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS {self.table_name} (\n        id SERIAL PRIMARY KEY,\n        session_id TEXT NOT NULL,\n        message JSONB NOT NULL\n    );\"\"\"\n    self.cursor.execute(create_table_query)\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Add a message to the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Add a message to the database.\"\"\"\n    from psycopg2 import sql\n\n    query = sql.SQL(\"INSERT INTO {} (session_id, message) VALUES (%s, %s);\").format(\n        sql.Identifier(self.table_name)\n    )\n    self.cursor.execute(query, (self.session_id, json.dumps(message.dict())))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from the database.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from the database.\"\"\"\n    query = f\"DELETE FROM {self.table_name} WHERE session_id = %s;\"\n    self.cursor.execute(query, (self.session_id,))\n    self.conn.commit()\n</code></pre>"},{"location":"reference/declarai/memory/postgres/#declarai.memory.postgres.PostgresMessageHistory.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close cursor and connection.</p> Source code in <code>src/declarai/memory/postgres.py</code> <pre><code>def close(self):\n\"\"\"Close cursor and connection.\"\"\"\n    self.cursor.close()\n    self.conn.close()\n</code></pre>"},{"location":"reference/declarai/memory/redis/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> redis","text":""},{"location":"reference/declarai/memory/redis/#declarai.memory.redis","title":"redis","text":"<p>This module contains the RedisMessageHistory class, which is used to store chat message history in a Redis database.</p> <p>Classes:</p> Name Description <code>RedisMessageHistory</code> <p>Chat message history that stores history in a Redis database.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT_TABLE_NAME</code> <p>A table name for the Redis database.</p> <code>DEFAULT_URL</code> <p>A URL for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.DEFAULT_TABLE_NAME","title":"DEFAULT_TABLE_NAME  <code>module-attribute</code>","text":"<pre><code>DEFAULT_TABLE_NAME = 'message_store'\n</code></pre> <p>A table name for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.DEFAULT_URL","title":"DEFAULT_URL  <code>module-attribute</code>","text":"<pre><code>DEFAULT_URL = 'redis://localhost:6379/0'\n</code></pre> <p>A URL for the Redis database.</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory","title":"RedisMessageHistory","text":"<pre><code>RedisMessageHistory(\n    session_id: str,\n    url: str = DEFAULT_URL,\n    key_prefix: str = f\"{DEFAULT_TABLE_NAME}:\",\n    ttl: Optional[int] = None,\n)\n</code></pre> <p>             Bases: <code>BaseChatMessageHistory</code></p> <p>Chat message history that stores history in a Redis database.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>Arbitrary key that is used to store the messages for a single chat session.</p> required <code>url</code> <code>str</code> <p>URL to connect to the Redis server.</p> <code>DEFAULT_URL</code> <code>key_prefix</code> <code>str</code> <p>Prefix for the Redis key.</p> <code>f'{DEFAULT_TABLE_NAME}:'</code> <code>ttl</code> <code>Optional[int]</code> <p>Time-to-live for the message records.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_message</code> <p>Append the message to the record in Redis</p> <code>clear</code> <p>Clear session memory from Redis</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>Construct the record key to use</p> <code>history</code> <code>List[Message]</code> <p>Retrieve the messages from Redis</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def __init__(\n    self,\n    session_id: str,\n    url: str = DEFAULT_URL,\n    key_prefix: str = f\"{DEFAULT_TABLE_NAME}:\",\n    ttl: Optional[int] = None,\n):\n    super().__init__()\n    try:\n        import redis  # pylint: disable=import-outside-toplevel\n    except ImportError:\n        raise ImportError(\n            \"Could not import redis python package. \"\n            \"Please install it with `pip install redis`.\"\n        )\n\n    self.redis_client = redis.StrictRedis.from_url(url)\n    self.session_id = session_id\n    self.key_prefix = key_prefix\n    self.ttl = ttl\n</code></pre>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Message]\n</code></pre> <p>Retrieve the messages from Redis</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.key","title":"key  <code>property</code>","text":"<pre><code>key: str\n</code></pre> <p>Construct the record key to use</p>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message) -&gt; None\n</code></pre> <p>Append the message to the record in Redis</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def add_message(self, message: Message) -&gt; None:\n\"\"\"Append the message to the record in Redis\"\"\"\n    self.redis_client.lpush(self.key, json.dumps(message.dict()))\n    if self.ttl:\n        self.redis_client.expire(self.key, self.ttl)\n</code></pre>"},{"location":"reference/declarai/memory/redis/#declarai.memory.redis.RedisMessageHistory.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear session memory from Redis</p> Source code in <code>src/declarai/memory/redis.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear session memory from Redis\"\"\"\n    self.redis_client.delete(self.key)\n</code></pre>"},{"location":"reference/declarai/middleware/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> middleware","text":""},{"location":"reference/declarai/middleware/#declarai.middleware","title":"middleware","text":"<p>Modules:</p> Name Description <code>base</code> <p>Base class for task middlewares.</p>"},{"location":"reference/declarai/middleware/base/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base","text":""},{"location":"reference/declarai/middleware/base/#declarai.middleware.base","title":"base","text":"<p>Base class for task middlewares.</p> <p>Classes:</p> Name Description <code>TaskMiddleware</code> <p>Base class for task middlewares. Middlewares are used to wrap a task and perform some actions before and after the task is executed.</p>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware","title":"TaskMiddleware","text":"<pre><code>TaskMiddleware(task: TaskType, kwargs: TaskType)\n</code></pre> <p>Base class for task middlewares. Middlewares are used to wrap a task and perform some actions before and after the task is executed. Is mainly used for logging, but can be used for other purposes as well. Please see <code>LoggingMiddleware</code> for an example of a middleware.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>TaskType</code> <p>The task to wrap</p> required <code>kwargs</code> <p>The keyword arguments to pass to the task</p> required <p>Attributes:</p> Name Type Description <code>_task</code> <p>The task to wrap</p> <code>_kwargs</code> <p>The keyword arguments to pass to the task</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Once the middleware is called, it executes the task and returns the result.</p> <code>before</code> <p>Executed before the task is executed. Should be used to perform some actions before the task is executed.</p> <code>after</code> <p>Executed after the task is executed. Should be used to perform some actions after the task is executed.</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __init__(self, task: TaskType, kwargs):\n    self._task = task\n    self._kwargs = kwargs\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Once the middleware is called, it executes the task and returns the result. Before it executes the task, it calls the <code>before</code> method, and after it executes the task, it calls the <code>after</code> method.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the task</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Once the middleware is called, it executes the task and returns the result.\n    Before it executes the task, it calls the `before` method, and after it executes the task, it calls the `after` method.\n    Returns:\n        The result of the task\n    \"\"\"\n    self.before(self._task)\n    res = self._task._exec(self._kwargs)\n    self.after(self._task)\n    return res\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.after","title":"after  <code>abstractmethod</code>","text":"<pre><code>after(task: TaskType) -&gt; None\n</code></pre> <p>Executed after the task is executed. Should be used to perform some actions after the task is executed.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>TaskType</code> <p>the task to execute</p> required Source code in <code>src/declarai/middleware/base.py</code> <pre><code>@abstractmethod\ndef after(self, task: TaskType) -&gt; None:\n\"\"\"\n    Executed after the task is executed. Should be used to perform some actions after the task is executed.\n    Args:\n        task: the task to execute\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/middleware/base/#declarai.middleware.base.TaskMiddleware.before","title":"before  <code>abstractmethod</code>","text":"<pre><code>before(task: TaskType) -&gt; None\n</code></pre> <p>Executed before the task is executed. Should be used to perform some actions before the task is executed.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>TaskType</code> <p>the task to execute</p> required Source code in <code>src/declarai/middleware/base.py</code> <pre><code>@abstractmethod\ndef before(self, task: TaskType) -&gt; None:\n\"\"\"\n    Executed before the task is executed. Should be used to perform some actions before the task is executed.\n    Args:\n        task: the task to execute\n    \"\"\"\n</code></pre>"},{"location":"reference/declarai/middleware/internal/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> internal","text":""},{"location":"reference/declarai/middleware/internal/#declarai.middleware.internal","title":"internal","text":"<p>Modules:</p> Name Description <code>log_middleware</code> <p>Logger Middleware</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> log_middleware","text":""},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware","title":"log_middleware","text":"<p>Logger Middleware</p> <p>Classes:</p> Name Description <code>LoggingMiddleware</code> <p>Creates a Simple logging middleware for a given task.</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware","title":"LoggingMiddleware","text":"<p>             Bases: <code>TaskMiddleware</code></p> <p>Creates a Simple logging middleware for a given task.</p> Example <pre><code>@declarai.task(middlewares=[LoggingMiddleware])\ndef generate_a_poem(title: str):\n'''\n    Generate a poem based on the given title\n    :return: The generated poem\n    '''\n    return declarai.magic(\"poem\", title)\n</code></pre> <p>Methods:</p> Name Description <code>__call__</code> <p>Once the middleware is called, it executes the task and returns the result.</p> <code>before</code> <p>Before execution of the task, set the start time.</p> <code>after</code> <p>After execution of the task, log the task details.</p>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.__call__","title":"__call__","text":"<pre><code>__call__() -&gt; Any\n</code></pre> <p>Once the middleware is called, it executes the task and returns the result. Before it executes the task, it calls the <code>before</code> method, and after it executes the task, it calls the <code>after</code> method.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the task</p> Source code in <code>src/declarai/middleware/base.py</code> <pre><code>def __call__(self) -&gt; Any:\n\"\"\"\n    Once the middleware is called, it executes the task and returns the result.\n    Before it executes the task, it calls the `before` method, and after it executes the task, it calls the `after` method.\n    Returns:\n        The result of the task\n    \"\"\"\n    self.before(self._task)\n    res = self._task._exec(self._kwargs)\n    self.after(self._task)\n    return res\n</code></pre>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.after","title":"after","text":"<pre><code>after(task: TaskType)\n</code></pre> <p>After execution of the task, log the task details.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>TaskType</code> <p>the task to be logged</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the task details like execution time, task name, template, compiled template, result, time.</p> Source code in <code>src/declarai/middleware/internal/log_middleware.py</code> <pre><code>def after(self, task: TaskType):\n\"\"\"\n    After execution of the task, log the task details.\n    Args:\n        task: the task to be logged\n\n    Returns:\n        (Dict[str, Any]): the task details like execution time, task name, template, compiled template, result, time.\n\n    \"\"\"\n    end_time = time() - self.start_time\n    log_record = {\n        \"task_name\": task.__name__,\n        \"llm_model\": task.llm_response.model,\n        \"template\": str(task.compile()),\n        \"call_kwargs\": str(task._kwargs),\n        \"compiled_template\": str(task.compile(**task._kwargs)),\n        \"result\": task.llm_response.response,\n        \"time\": end_time,\n    }\n    logger.info(log_record)\n</code></pre>"},{"location":"reference/declarai/middleware/internal/log_middleware/#declarai.middleware.internal.log_middleware.LoggingMiddleware.before","title":"before","text":"<pre><code>before(_)\n</code></pre> <p>Before execution of the task, set the start time.</p> Source code in <code>src/declarai/middleware/internal/log_middleware.py</code> <pre><code>def before(self, _):\n\"\"\"\n    Before execution of the task, set the start time.\n    \"\"\"\n    self.start_time = time()\n</code></pre>"},{"location":"reference/declarai/middleware/third_party/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> third_party","text":""},{"location":"reference/declarai/middleware/third_party/#declarai.middleware.third_party","title":"third_party","text":"<p>Modules:</p> Name Description <code>wandb_monitor</code> <p>Wandb Monitor Middleware used to monitor the execution on wandb.</p>"},{"location":"reference/declarai/middleware/third_party/wandb_monitor/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> wandb_monitor","text":""},{"location":"reference/declarai/middleware/third_party/wandb_monitor/#declarai.middleware.third_party.wandb_monitor","title":"wandb_monitor","text":"<p>Wandb Monitor Middleware used to monitor the execution on wandb.</p> <p>Classes:</p> Name Description <code>WandDBMonitorCreator</code> <p>Creates a WandDBMonitor middleware for a given task.</p>"},{"location":"reference/declarai/middleware/third_party/wandb_monitor/#declarai.middleware.third_party.wandb_monitor.WandDBMonitorCreator","title":"WandDBMonitorCreator","text":"<p>Creates a WandDBMonitor middleware for a given task.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the run on wandb</p> required <code>project</code> <code>str</code> <p>The name of the project on wandb</p> required <code>key</code> <code>str</code> <p>The api key for wandb</p> required <p>Returns:</p> Type Description <code>WandDBMonitor</code> <p>A WandDBMonitor middleware</p> Example <pre><code>WandDBMonitor = WandDBMonitorCreator(\n     name=\"&lt;task name&gt;\",\n     project=\"&lt;project name&gt;\",\n     key=\"&lt;decorators-key&gt;\",\n )\n\n @declarai.task(middlewares=[WandDBMonitor])\n def generate_a_poem(title: str):\n'''\n     Generate a poem based on the given title\n     :return: The generated poem\n     '''\n     return declarai.magic(\"poem\", title)\n</code></pre>"},{"location":"reference/declarai/operators/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> operators","text":""},{"location":"reference/declarai/operators/#declarai.operators","title":"operators","text":"<p>Modules:</p> Name Description <code>operator</code> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <code>message</code> <p>Message definition for the operators.</p> <code>llm</code> <p>This module defines the base classes for the LLM interface.</p> <p>Functions:</p> Name Description <code>resolve_operator</code> <p>Resolves the operator to be used for the given llm_settings</p>"},{"location":"reference/declarai/operators/#declarai.operators.resolve_operator","title":"resolve_operator","text":"<pre><code>resolve_operator(\n    llm_settings: LLMSettings,\n    operator_type: Literal[\"chat\", \"task\"],\n    **kwargs: Literal[\"chat\", \"task\"]\n) -&gt; Tuple[Type[BaseOperator], LLM]\n</code></pre> <p>Resolves the operator to be used for the given llm_settings</p> <p>Parameters:</p> Name Type Description Default <code>llm_settings</code> <code>LLMSettings</code> <p>llm settings like provider, model, etc</p> required <code>operator_type</code> <code>Literal['chat', 'task']</code> <p>relevant operator type</p> required <code>kwargs</code> <p>api keys, etc</p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[BaseOperator]</code> <p>an operator class object of type BaseOperator</p> <code>LLM</code> <p>an llm class object of type LLM</p> Source code in <code>src/declarai/operators/__init__.py</code> <pre><code>def resolve_operator(\n    llm_settings: LLMSettings, operator_type: Literal[\"chat\", \"task\"], **kwargs\n) -&gt; Tuple[Type[BaseOperator], LLM]:\n\"\"\"\n    Resolves the operator to be used for the given llm_settings\n\n    Args:\n        llm_settings: llm settings like provider, model, etc\n        operator_type: relevant operator type\n        kwargs: api keys, etc\n\n    Returns:\n        an operator class object of type BaseOperator\n        an llm class object of type LLM\n\n    \"\"\"\n    if llm_settings.provider == \"openai\":\n        open_ai_token = kwargs.get(\"openai_token\")\n        model = llm_settings.model\n        if open_ai_token:\n            llm = OpenAILLM(\n                model=model,\n                openai_token=open_ai_token,\n            )\n        else:\n            llm = OpenAILLM(model=model)\n\n        if operator_type == \"task\":\n            operator = OpenAITaskOperator\n        elif operator_type == \"chat\":\n            operator = OpenAIChatOperator\n        else:\n            raise NotImplementedError(\n                f\"Operator type : {operator_type} not implemented\"\n            )\n        return operator, llm\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/declarai/operators/llm/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> llm","text":""},{"location":"reference/declarai/operators/llm/#declarai.operators.llm","title":"llm","text":"<p>This module defines the base classes for the LLM interface.</p> <p>Classes:</p> Name Description <code>LLMResponse</code> <p>The response from the LLM.</p> <code>BaseLLMParams</code> <p>The base LLM params that are common to all LLMs.</p> <code>LLMSettings</code> <p>The settings for the LLM. Defines the model and version to use.</p> <code>BaseLLM</code> <p>The base LLM class that all LLMs should inherit from.</p> <p>Attributes:</p> Name Type Description <code>LLMParamsType</code> <p>Type variable for LLM params</p> <code>LLM</code> <p>Type variable for LLM</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLM","title":"LLM  <code>module-attribute</code>","text":"<pre><code>LLM = TypeVar('LLM', bound=BaseLLM)\n</code></pre> <p>Type variable for LLM</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMParamsType","title":"LLMParamsType  <code>module-attribute</code>","text":"<pre><code>LLMParamsType = TypeVar(\n    \"LLMParamsType\", bound=BaseLLMParams\n)\n</code></pre> <p>Type variable for LLM params</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLM","title":"BaseLLM","text":"<p>The base LLM class that all LLMs should inherit from.</p> <p>Methods:</p> Name Description <code>predict</code> <p>The predict method that all LLMs should implement.</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLM.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs) -&gt; LLMResponse\n</code></pre> <p>The predict method that all LLMs should implement.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>()</code> <code>**kwargs</code> <code>{}</code> Source code in <code>src/declarai/operators/llm.py</code> <pre><code>@abstractmethod\ndef predict(self, *args, **kwargs) -&gt; LLMResponse:\n\"\"\"\n    The predict method that all LLMs should implement.\n    Args:\n        *args:\n        **kwargs:\n\n    Returns: llm response object\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.BaseLLMParams","title":"BaseLLMParams","text":"<p>             Bases: <code>TypedDict</code></p> <p>The base LLM params that are common to all LLMs.</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMResponse","title":"LLMResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>The response from the LLM.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The raw response from the LLM</p> <code>model</code> <code>Optional[str]</code> <p>The model that was used to generate the response</p> <code>prompt_tokens</code> <code>Optional[int]</code> <p>The number of tokens in the prompt</p> <code>completion_tokens</code> <code>Optional[int]</code> <p>The number of tokens in the completion</p> <code>total_tokens</code> <code>Optional[int]</code> <p>The total number of tokens in the response</p>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMSettings","title":"LLMSettings","text":"<pre><code>LLMSettings(\n    provider: str,\n    model: str,\n    version: Optional[str] = None,\n    **_: Optional[str]\n)\n</code></pre> <p>The settings for the LLM. Defines the model and version to use.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The provider of the model (openai, cohere, etc.)</p> required <code>model</code> <code>str</code> <p>The model to use (gpt-4, gpt-3.5-turbo, etc.)</p> required <code>version</code> <code>Optional[str]</code> <p>The version of the model to use (optional)</p> <code>None</code> <code>**_</code> <p>Any additional params that are specific to the provider that will be ignored.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>provider</code> <code>str</code> <p>The provider of the model (openai, cohere, etc.)</p> <code>model</code> <code>str</code> <p>The full model name to use.</p> <code>version</code> <p>The version of the model to use (optional)</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Some model providers allow defining a base model as well as a sub-model.</p> Source code in <code>src/declarai/operators/llm.py</code> <pre><code>def __init__(\n    self,\n    provider: str,\n    model: str,\n    version: Optional[str] = None,\n    **_,\n):\n    self.provider = provider\n    self._model = model\n    self.version = version\n</code></pre>"},{"location":"reference/declarai/operators/llm/#declarai.operators.llm.LLMSettings.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Some model providers allow defining a base model as well as a sub-model. Often the base model is an alias to latest model served on that model. for example, when sending gpt-3.5-turbo to OpenAI, the actual model will be one of the publicly available snapshots or an internally exposed version as described on their website: as of 27/07/2023 - https://platform.openai.com/docs/models/continuous-model-upgrades | With the release of gpt-3.5-turbo, some of our models are now being continually updated. | We also offer static model versions that developers can continue using for at least | three months after an updated model has been introduced.</p> <p>Another use-case for sub models is using your own fine-tuned models. As described in the documentation: https://platform.openai.com/docs/guides/fine-tuning/customize-your-model-name</p> <p>You will likely build your fine-tuned model names by concatenating the base model name with the fine-tuned model name, separated by a hyphen. For example gpt-3.5-turbo-declarai-text-classification-2023-03 or gpt-3.5-turbo:declarai:text-classification-2023-03</p> <p>In any case you can always pass the full model name in the model parameter and leave the sub_model parameter empty if you prefer.</p>"},{"location":"reference/declarai/operators/message/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> message","text":""},{"location":"reference/declarai/operators/message/#declarai.operators.message","title":"message","text":"<p>Message definition for the operators.</p> <p>Classes:</p> Name Description <code>MessageRole</code> <p>Message role enum for the Message class to indicate the role of the message in the chat.</p> <code>Message</code> <p>Represents a message in the chat.</p>"},{"location":"reference/declarai/operators/message/#declarai.operators.message.Message","title":"Message","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents a message in the chat.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <p>The message string</p> required <code>role</code> <p>The role of the message in the chat</p> required <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>The message string</p> <code>role</code> <code>MessageRole</code> <p>The role of the message in the chat</p>"},{"location":"reference/declarai/operators/message/#declarai.operators.message.MessageRole","title":"MessageRole","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enum for the Message class to indicate the role of the message in the chat.</p> <p>Attributes:</p> Name Type Description <code>system</code> <code>str</code> <p>The message is the system message, usually used as the first message in the chat.</p> <code>user</code> <code>str</code> <p>Every message that is sent by the user.</p> <code>assistant</code> <code>str</code> <p>Every message that is sent by the assistant.</p> <code>function</code> <code>str</code> <p>Every message that is sent by the assistant that is a function call.</p>"},{"location":"reference/declarai/operators/operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> operator","text":""},{"location":"reference/declarai/operators/operator/#declarai.operators.operator","title":"operator","text":"<p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <p>Classes:</p> Name Description <code>BaseOperator</code> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <code>BaseChatOperator</code> <p>Base class for chat operators.</p>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator","title":"BaseChatOperator","text":"<pre><code>BaseChatOperator(\n    system: str,\n    greeting: Optional[str] = None,\n    **kwargs: Optional[str]\n)\n</code></pre> <p>             Bases: <code>BaseOperator</code></p> <p>Base class for chat operators. It extends the <code>BaseOperator</code> class and adds additional attributes that are used for chat operators. See <code>BaseOperator</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>The system message that is used for the chat</p> required <code>greeting</code> <code>Optional[str]</code> <p>The greeting message that is used for the chat.</p> <code>None</code> <code>kwargs</code> <p>Enables passing all the required parameters for <code>BaseOperator</code></p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>system</code> <code>str</code> <p>The system message that is used for the chat</p> <code>greeting</code> <code>str</code> <p>The greeting message that is used for the chat.</p> <code>parsed_send_func</code> <code>PythonParser</code> <p>The parsed object that is used to compile the send function.</p> <p>Methods:</p> Name Description <code>compile</code> <p>An abstract method that compiles the prompts using the parsed object and returns the compiled prompts.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def __init__(self, system: str, greeting: Optional[str] = None, **kwargs):\n    super().__init__(**kwargs)\n    self.system = system or self.parsed.docstring_freeform\n    self.greeting = greeting or getattr(self.parsed.decorated, \"greeting\", None)\n    self.parsed_send_func = (\n        PythonParser(self.parsed.decorated.send)\n        if getattr(self.parsed.decorated, \"send\", None)\n        else None\n    )\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.compile","title":"compile  <code>abstractmethod</code>","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>An abstract method that compiles the prompts using the parsed object and returns the compiled prompts. The implementation of this method should be specific to the operator, and should be implemented in the child class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Any runtime parameters that are passed to the operator. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>The compiled prompts that can be directly passed to the <code>predict</code> method of the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>@abstractmethod\ndef compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    An abstract method that compiles the prompts using the parsed object and returns the compiled prompts.\n    The implementation of this method should be specific to the operator, and should be implemented in the child class.\n    Args:\n        **kwargs: Any runtime parameters that are passed to the operator. Used to format the prompts placeholders.\n\n    Returns:\n        The compiled prompts that can be directly passed to the `predict` method of the LLM\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>llm string output</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseChatOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; LLMResponse\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params.</p> <p>Parameters:</p> Name Type Description Default <code>llm_params</code> <code>Optional[LLMParamsType]</code> <p>The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; LLMResponse:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator","title":"BaseOperator","text":"<pre><code>BaseOperator(\n    llm: LLM,\n    parsed: PythonParser,\n    llm_params: LLMParamsType = None,\n    **kwargs: Dict\n)\n</code></pre> <p>Operator is a class that is used to wrap the compilation of prompts and the singular execution of the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>The LLM to use for the operator</p> required <code>parsed</code> <code>PythonParser</code> <p>The parsed object that is used to compile the prompts</p> required <code>llm_params</code> <code>LLMParamsType</code> <p>The parameters to pass to the LLM</p> <code>None</code> <code>kwargs</code> <code>Dict</code> <p>Enables passing of additional parameters to the operator</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>llm</code> <code>LLM</code> <p>The LLM to use for the operator</p> <code>parsed</code> <code>PythonParser</code> <p>The parsed object that is used to compile the prompts</p> <code>llm_params</code> <code>LLMParamsType</code> <p>The parameters that were passed during initialization of the operator</p> <p>Methods:</p> Name Description <code>compile</code> <p>Compiles the prompts using the parsed object and returns the compiled prompts</p> <code>predict</code> <p>Executes the LLM with the compiled prompts and the llm_params</p> <code>parse_output</code> <p>Parses the output of the LLM</p> <p>Methods:</p> Name Description <code>compile</code> <p>An abstract method that compiles the prompts using the parsed object and returns the compiled prompts.</p> <code>predict</code> <p>Executes prediction using the LLM.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def __init__(\n    self,\n    llm: LLM,\n    parsed: PythonParser,\n    llm_params: LLMParamsType = None,\n    **kwargs: Dict,\n):\n    self.llm = llm\n    self.parsed = parsed\n    self.llm_params = llm_params or {}\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.compile","title":"compile  <code>abstractmethod</code>","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>An abstract method that compiles the prompts using the parsed object and returns the compiled prompts. The implementation of this method should be specific to the operator, and should be implemented in the child class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Any runtime parameters that are passed to the operator. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>The compiled prompts that can be directly passed to the <code>predict</code> method of the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>@abstractmethod\ndef compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    An abstract method that compiles the prompts using the parsed object and returns the compiled prompts.\n    The implementation of this method should be specific to the operator, and should be implemented in the child class.\n    Args:\n        **kwargs: Any runtime parameters that are passed to the operator. Used to format the prompts placeholders.\n\n    Returns:\n        The compiled prompts that can be directly passed to the `predict` method of the LLM\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>llm string output</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/operator/#declarai.operators.operator.BaseOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; LLMResponse\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params.</p> <p>Parameters:</p> Name Type Description Default <code>llm_params</code> <code>Optional[LLMParamsType]</code> <p>The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; LLMResponse:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai_operators","text":""},{"location":"reference/declarai/operators/openai_operators/#declarai.operators.openai_operators","title":"openai_operators","text":"<p>Modules:</p> Name Description <code>settings</code> <p>Environment level configurations for working with openai provider.</p> <code>chat_operator</code> <p>Chat implementation of OpenAI operator.</p> <code>openai_llm</code> <p>LLM implementation for OpenAI</p> <code>task_operator</code> <p>Task implementation for openai operator.</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chat_operator","text":""},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator","title":"chat_operator","text":"<p>Chat implementation of OpenAI operator.</p> <p>Classes:</p> Name Description <code>OpenAIChatOperator</code> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator","title":"OpenAIChatOperator","text":"<p>             Bases: <code>BaseChatOperator</code></p> <p>Chat implementation of OpenAI operator. This is a child of the BaseChatOperator class. See the BaseChatOperator class for further documentation.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>OpenAILLM</code> <p>OpenAILLM</p> <p>Methods:</p> Name Description <code>predict</code> <p>Executes prediction using the LLM.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>compile</code> <p>Implementation of the compile method for the chat operator.</p>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.compile","title":"compile","text":"<pre><code>compile(\n    messages: List[Message], **kwargs: List[Message]\n) -&gt; CompiledTemplate\n</code></pre> <p>Implementation of the compile method for the chat operator. Compiles a system prompt based on the initialized system message Compiles the message based on the user input and the StructuredOutputChatPrompt template</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages</p> required <code>**kwargs</code> <code>{}</code> Source code in <code>src/declarai/operators/openai_operators/chat_operator.py</code> <pre><code>def compile(self, messages: List[Message], **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    Implementation of the compile method for the chat operator.\n    Compiles a system prompt based on the initialized system message\n    Compiles the message based on the user input and the StructuredOutputChatPrompt template\n    Args:\n        messages (List[Message]): A list of messages\n        **kwargs:\n\n    Returns:\n\n    \"\"\"\n    self.system = self.system.format(**kwargs)\n    structured_template = StructuredOutputChatPrompt\n    if self.parsed_send_func:\n        output_schema = self._compile_output_prompt(structured_template)\n    else:\n        output_schema = None\n\n    if output_schema:\n        compiled_system_prompt = f\"{self.system}/n{output_schema}\"\n    else:\n        compiled_system_prompt = self.system\n    messages = [\n        Message(message=compiled_system_prompt, role=MessageRole.system)\n    ] + messages\n    return {\"messages\": messages}\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>llm string output</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/chat_operator/#declarai.operators.openai_operators.chat_operator.OpenAIChatOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; LLMResponse\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params.</p> <p>Parameters:</p> Name Type Description Default <code>llm_params</code> <code>Optional[LLMParamsType]</code> <p>The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; LLMResponse:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> openai_llm","text":""},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm","title":"openai_llm","text":"<p>LLM implementation for OpenAI</p> <p>Classes:</p> Name Description <code>OpenAIError</code> <p>Generic OpenAI error class when working with OpenAI provider.</p> <code>OpenAILLM</code> <p>OpenAI LLM implementation that uses openai sdk to make predictions.</p> <code>OpenAILLMParams</code> <p>OpenAI LLM Params when running execution</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAIError","title":"OpenAIError","text":"<p>             Bases: <code>Exception</code></p> <p>Generic OpenAI error class when working with OpenAI provider.</p>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLM","title":"OpenAILLM","text":"<pre><code>OpenAILLM(openai_token: str = None, model: str = None)\n</code></pre> <p>             Bases: <code>BaseLLM</code></p> <p>OpenAI LLM implementation that uses openai sdk to make predictions.</p> <p>Parameters:</p> Name Type Description Default <code>openai_token</code> <code>str</code> <p>OpenAI API key</p> <code>None</code> <code>model</code> <code>str</code> <p>OpenAI model name</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>openai</code> <code>openai</code> <p>OpenAI SDK</p> <code>model</code> <code>str</code> <p>OpenAI model name</p> <p>Methods:</p> Name Description <code>predict</code> <p>Predicts the next message using OpenAI</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def __init__(self, openai_token: str = None, model: str = None):\n    self.openai = openai\n    self.openai.api_key = openai_token or OPENAI_API_KEY\n    if not self.openai.api_key:\n        raise OpenAIError(\n            \"Missing an OpenAI API key\"\n            \"In order to work with OpenAI, you will need to provide an API key\"\n            \"either by setting the DECLARAI_OPENAI_API_KEY or by providing\"\n            \"the API key via the init interface.\"\n        )\n    self.model = model or OPENAI_MODEL\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLM.predict","title":"predict","text":"<pre><code>predict(\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n) -&gt; LLMResponse\n</code></pre> <p>Predicts the next message using OpenAI</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>List of messages that are used as context for the prediction</p> required <code>model</code> <code>str</code> <p>the model to use for the prediction</p> <code>None</code> <code>temperature</code> <code>float</code> <p>the temperature to use for the prediction</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>the maximum number of tokens to use for the prediction</p> <code>3000</code> <code>top_p</code> <code>float</code> <p>the top p to use for the prediction</p> <code>1</code> <code>frequency_penalty</code> <code>int</code> <p>the frequency penalty to use for the prediction</p> <code>0</code> <code>presence_penalty</code> <code>int</code> <p>the presence penalty to use for the prediction</p> <code>0</code> <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>LLMResponse</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/openai_operators/openai_llm.py</code> <pre><code>def predict(\n    self,\n    messages: List[Message],\n    model: str = None,\n    temperature: float = 0,\n    max_tokens: int = 3000,\n    top_p: float = 1,\n    frequency_penalty: int = 0,\n    presence_penalty: int = 0,\n) -&gt; LLMResponse:\n\"\"\"\n    Predicts the next message using OpenAI\n    Args:\n        messages: List of messages that are used as context for the prediction\n        model: the model to use for the prediction\n        temperature: the temperature to use for the prediction\n        max_tokens: the maximum number of tokens to use for the prediction\n        top_p: the top p to use for the prediction\n        frequency_penalty: the frequency penalty to use for the prediction\n        presence_penalty: the presence penalty to use for the prediction\n\n    Returns:\n        LLMResponse: The response from the LLM\n\n    \"\"\"\n    openai_messages = [{\"role\": m.role, \"content\": m.message} for m in messages]\n    res = self.openai.ChatCompletion.create(\n        model=model or self.model,\n        messages=openai_messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty,\n    )\n    return LLMResponse(\n        response=res.choices[0][\"message\"][\"content\"],\n        model=res.model,\n        prompt_tokens=res[\"usage\"][\"prompt_tokens\"],\n        completion_tokens=res[\"usage\"][\"completion_tokens\"],\n        total_tokens=res[\"usage\"][\"total_tokens\"],\n    )\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/openai_llm/#declarai.operators.openai_operators.openai_llm.OpenAILLMParams","title":"OpenAILLMParams","text":"<p>             Bases: <code>BaseLLMParams</code></p> <p>OpenAI LLM Params when running execution</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>Optional[float]</code> <p>the temperature to use for the prediction</p> <code>max_tokens</code> <code>Optional[int]</code> <p>the maximum number of tokens to use for the prediction</p> <code>top_p</code> <code>Optional[float]</code> <p>the top p to use for the prediction</p> <code>frequency_penalty</code> <code>Optional[int]</code> <p>the frequency penalty to use for the prediction</p> <code>presence_penalty</code> <code>Optional[int]</code> <p>the presence penalty to use for the prediction</p>"},{"location":"reference/declarai/operators/openai_operators/settings/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> settings","text":""},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings","title":"settings","text":"<p>Environment level configurations for working with openai provider.</p> <p>Attributes:</p> Name Type Description <code>OPENAI_API_KEY</code> <code>str</code> <p>API key for openai provider.</p> <code>OPENAI_MODEL</code> <code>str</code> <p>Model name for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.OPENAI_API_KEY","title":"OPENAI_API_KEY  <code>module-attribute</code>","text":"<pre><code>OPENAI_API_KEY: str = os.getenv(\n    f\"{DECLARAI_PREFIX}_OPENAI_API_KEY\", \"\"\n)\n</code></pre> <p>API key for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/settings/#declarai.operators.openai_operators.settings.OPENAI_MODEL","title":"OPENAI_MODEL  <code>module-attribute</code>","text":"<pre><code>OPENAI_MODEL: str = os.getenv(\n    f\"{DECLARAI_PREFIX}_OPENAI_MODEL\", \"gpt-3.5-turbo\"\n)\n</code></pre> <p>Model name for openai provider.</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> task_operator","text":""},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator","title":"task_operator","text":"<p>Task implementation for openai operator.</p> <p>Classes:</p> Name Description <code>OpenAITaskOperator</code> <p>Task implementation for openai operator. This is a child of the BaseOperator class. See the BaseOperator class for further documentation.</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator","title":"OpenAITaskOperator","text":"<p>             Bases: <code>BaseOperator</code></p> <p>Task implementation for openai operator. This is a child of the BaseOperator class. See the BaseOperator class for further documentation. Implements the compile method which compiles a parsed function into a message. Uses the OpenAILLM to generate a response based on the given template.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>OpenAILLM</code> <p>OpenAILLM</p> <p>Methods:</p> Name Description <code>predict</code> <p>Executes prediction using the LLM.</p> <code>parse_output</code> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <code>_compile_input_placeholder</code> <p>Creates a placeholder for the input of the function.</p> <code>compile_template</code> <p>Unique compilation method for the OpenAITaskOperator class.</p> <code>compile</code> <p>Implements the compile method of the BaseOperator class.</p>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator._compile_input_placeholder","title":"_compile_input_placeholder","text":"<pre><code>_compile_input_placeholder() -&gt; str\n</code></pre> <p>Creates a placeholder for the input of the function. The input format is based on the function input schema.</p> <p>Example</p> <p>for example a function signature of:     <pre><code>def foo(a: int, b: str, c: float = 1.0):\n</code></pre></p> <p>will result in the following placeholder: <pre><code>    Inputs:\n    a: {a}\n    b: {b}\n    c: {c}\n</code></pre></p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def _compile_input_placeholder(self) -&gt; str:\n\"\"\"\n    Creates a placeholder for the input of the function.\n    The input format is based on the function input schema.\n\n    !!! example\n        for example a function signature of:\n            ```py\n            def foo(a: int, b: str, c: float = 1.0):\n            ```\n\n        will result in the following placeholder:\n        ```md\n            Inputs:\n            a: {a}\n            b: {b}\n            c: {c}\n        ```\n    \"\"\"\n    inputs = \"\"\n\n    if not self.parsed.signature_kwargs.keys():\n        return inputs\n\n    for i, param in enumerate(self.parsed.signature_kwargs.keys()):\n        if i == 0:\n            inputs += INPUT_LINE_TEMPLATE.format(param=param)\n            continue\n        inputs += NEW_LINE_INPUT_LINE_TEMPLATE.format(param=param)\n\n    return INPUTS_TEMPLATE.format(inputs=inputs)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.compile","title":"compile","text":"<pre><code>compile(**kwargs) -&gt; CompiledTemplate\n</code></pre> <p>Implements the compile method of the BaseOperator class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def compile(self, **kwargs) -&gt; CompiledTemplate:\n\"\"\"\n    Implements the compile method of the BaseOperator class.\n    Args:\n        **kwargs:\n\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    template = self.compile_template()\n    if kwargs:\n        template[-1].message = template[-1].message.format(**kwargs)\n        return {\"messages\": template}\n    return {\"messages\": template}\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.compile_template","title":"compile_template","text":"<pre><code>compile_template() -&gt; CompiledTemplate\n</code></pre> <p>Unique compilation method for the OpenAITaskOperator class. Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message. And the _compile_input_placeholder method to create a placeholder for the input of the function.</p> <p>Returns:</p> Type Description <code>CompiledTemplate</code> <p>Dict[str, List[Message]]: A dictionary containing a list of messages.</p> Source code in <code>src/declarai/operators/openai_operators/task_operator.py</code> <pre><code>def compile_template(self) -&gt; CompiledTemplate:\n\"\"\"\n    Unique compilation method for the OpenAITaskOperator class.\n    Uses the InstructFunctionTemplate and StructuredOutputInstructionPrompt templates to create a message.\n    And the _compile_input_placeholder method to create a placeholder for the input of the function.\n    Returns:\n        Dict[str, List[Message]]: A dictionary containing a list of messages.\n\n    \"\"\"\n    instruction_template = InstructFunctionTemplate\n    structured_template = StructuredOutputInstructionPrompt\n    output_schema = self._compile_output_prompt(structured_template)\n\n    messages = []\n    if output_schema:\n        messages.append(Message(message=output_schema, role=MessageRole.system))\n\n    populated_instruction = instruction_template.format(\n        input_instructions=self.parsed.docstring_freeform,\n        input_placeholder=self._compile_input_placeholder(),\n    )\n    messages.append(Message(message=populated_instruction, role=MessageRole.user))\n    return messages\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.parse_output","title":"parse_output","text":"<pre><code>parse_output(output: str) -&gt; Any\n</code></pre> <p>Parses the raw output from the LLM into the desired format that was set in the parsed object.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>llm string output</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Any parsed output</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def parse_output(self, output: str) -&gt; Any:\n\"\"\"\n    Parses the raw output from the LLM into the desired format that was set in the parsed object.\n    Args:\n        output: llm string output\n\n    Returns:\n        Any parsed output\n    \"\"\"\n    return self.parsed.parse(output)\n</code></pre>"},{"location":"reference/declarai/operators/openai_operators/task_operator/#declarai.operators.openai_operators.task_operator.OpenAITaskOperator.predict","title":"predict","text":"<pre><code>predict(\n    *,\n    llm_params: Optional[LLMParamsType] = None,\n    **kwargs: object\n) -&gt; LLMResponse\n</code></pre> <p>Executes prediction using the LLM. It first compiles the prompts using the <code>compile</code> method, and then executes the LLM with the compiled prompts and the llm_params.</p> <p>Parameters:</p> Name Type Description Default <code>llm_params</code> <code>Optional[LLMParamsType]</code> <p>The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.</p> <code>None</code> <code>**kwargs</code> <code>object</code> <p>The keyword arguments to pass to the <code>compile</code> method. Used to format the prompts placeholders.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>The response from the LLM</p> Source code in <code>src/declarai/operators/operator.py</code> <pre><code>def predict(\n    self, *, llm_params: Optional[LLMParamsType] = None, **kwargs: object\n) -&gt; LLMResponse:\n\"\"\"\n    Executes prediction using the LLM.\n    It first compiles the prompts using the `compile` method, and then executes the LLM with the compiled prompts and the llm_params.\n    Args:\n        llm_params: The parameters that are passed during runtime. If provided, they will override the ones provided during initialization.\n        **kwargs: The keyword arguments to pass to the `compile` method. Used to format the prompts placeholders.\n\n    Returns:\n        The response from the LLM\n    \"\"\"\n    llm_params = llm_params or self.llm_params  # Order is important -\n    # provided params during execution should override the ones provided during initialization\n    return self.llm.predict(**self.compile(**kwargs), **llm_params)\n</code></pre>"},{"location":"reference/declarai/operators/templates/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> templates","text":""},{"location":"reference/declarai/operators/templates/#declarai.operators.templates","title":"templates","text":"<p>Modules:</p> Name Description <code>chain_of_thought</code> <p>Chain of thoughts templates.</p> <code>output_structure</code> <p>The prompt templates for the format of the output.</p> <code>instruct_function</code> <p>Instruct Function Template</p> <code>output_prompt</code> <p>The logic for constructing the output prompt.</p>"},{"location":"reference/declarai/operators/templates/chain_of_thought/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> chain_of_thought","text":""},{"location":"reference/declarai/operators/templates/chain_of_thought/#declarai.operators.templates.chain_of_thought","title":"chain_of_thought","text":"<p>Chain of thoughts templates.</p> <p>Attributes:</p> Name Type Description <code>ChainOfThoughtsTemplate</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/chain_of_thought/#declarai.operators.templates.chain_of_thought.ChainOfThoughtsTemplate","title":"ChainOfThoughtsTemplate  <code>module-attribute</code>","text":"<pre><code>ChainOfThoughtsTemplate = \"The following task should be done in {num_steps} steps:\\nUse the output of the previous step as the input of the next step.\\n{steps}\\n\\nLet's think step by step\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/instruct_function/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> instruct_function","text":""},{"location":"reference/declarai/operators/templates/instruct_function/#declarai.operators.templates.instruct_function","title":"instruct_function","text":"<p>Instruct Function Template</p> <p>Attributes:</p> Name Type Description <code>InstructFunctionTemplate</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/instruct_function/#declarai.operators.templates.instruct_function.InstructFunctionTemplate","title":"InstructFunctionTemplate  <code>module-attribute</code>","text":"<pre><code>InstructFunctionTemplate = (\n    \"{input_instructions}\\n{input_placeholder}\\n\"\n)\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_prompt/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> output_prompt","text":""},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt","title":"output_prompt","text":"<p>The logic for constructing the output prompt. These methods accept all \"return\" related properties of the python function and build a string output prompt from them.</p> <p>Functions:</p> Name Description <code>compile_unstructured_template</code> <p>Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float).</p> <code>compile_output_prompt</code> <p>Compiles the output prompt for given function properties.</p>"},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt.compile_output_prompt","title":"compile_output_prompt","text":"<pre><code>compile_output_prompt(\n    str_schema: str,\n    return_type: str,\n    return_docstring: str,\n    return_magic: str = None,\n    structured: Optional[bool] = True,\n    structured_template: Optional[str] = None,\n) -&gt; str\n</code></pre> <p>Compiles the output prompt for given function properties.</p> <p>Parameters:</p> Name Type Description Default <code>str_schema</code> <code>str</code> <p>tbd</p> required <code>return_type</code> <code>str</code> <p>tbd</p> required <code>return_docstring</code> <code>str</code> <p>tbd</p> required <code>return_magic</code> <code>str</code> <p>tbd</p> <code>None</code> <code>structured</code> <code>Optional[bool]</code> <p>tbd</p> <code>True</code> <code>structured_template</code> <code>Optional[str]</code> <p>tbd</p> <code>None</code> Source code in <code>src/declarai/operators/templates/output_prompt.py</code> <pre><code>def compile_output_prompt(\n    str_schema: str,\n    return_type: str,\n    return_docstring: str,\n    return_magic: str = None,\n    structured: Optional[bool] = True,\n    structured_template: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Compiles the output prompt for given function properties.\n    Args:\n        str_schema: tbd\n        return_type: tbd\n        return_docstring: tbd\n        return_magic: tbd\n        structured: tbd\n        structured_template: tbd\n\n    Returns:\n\n    \"\"\"\n    str_schema = str_schema or return_magic\n\n    if not structured:\n        return compile_unstructured_template(return_type, return_docstring)\n\n    return compile_output_schema_template(\n        str_schema, return_type, return_docstring, structured_template\n    )\n</code></pre>"},{"location":"reference/declarai/operators/templates/output_prompt/#declarai.operators.templates.output_prompt.compile_unstructured_template","title":"compile_unstructured_template","text":"<pre><code>compile_unstructured_template(\n    return_type: str, return_docstring: str\n) -&gt; str\n</code></pre> <p>Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float).</p> <p>Parameters:</p> Name Type Description Default <code>return_type</code> <code>str</code> <p>the type of the return value</p> required <code>return_docstring</code> <code>str</code> <p>the description of the return value</p> required Source code in <code>src/declarai/operators/templates/output_prompt.py</code> <pre><code>def compile_unstructured_template(return_type: str, return_docstring: str) -&gt; str:\n\"\"\"\n    Compiles the output prompt for unstructured output but where still a return type is expected (for example int, float).\n    Args:\n        return_type: the type of the return value\n        return_docstring: the description of the return value\n\n    Returns:\n\n    \"\"\"\n    if return_type == \"str\":\n        return \"\"\n    output_prompt = \"\"\n    if return_type:\n        output_prompt += f\"respond only with the value of type {return_type}:\"\n    if return_docstring:\n        output_prompt += f\"  # {return_docstring}\"\n\n    return output_prompt\n</code></pre>"},{"location":"reference/declarai/operators/templates/output_structure/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> output_structure","text":""},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure","title":"output_structure","text":"<p>The prompt templates for the format of the output.</p> <p>Attributes:</p> Name Type Description <code>StructuredOutputInstructionPrompt</code> <p>.</p> <code>StructuredOutputChatPrompt</code> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure.StructuredOutputChatPrompt","title":"StructuredOutputChatPrompt  <code>module-attribute</code>","text":"<pre><code>StructuredOutputChatPrompt = \"Your responses should be a JSON structure with a single key named '{return_name}', nothing else. The expected format is: {output_schema}\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/operators/templates/output_structure/#declarai.operators.templates.output_structure.StructuredOutputInstructionPrompt","title":"StructuredOutputInstructionPrompt  <code>module-attribute</code>","text":"<pre><code>StructuredOutputInstructionPrompt = \"You are a REST api endpoint.You only answer in JSON structures\\nwith a single key named '{return_name}', nothing else.\\nThe expected format is:\\n{output_schema}\"\n</code></pre> <p>.</p>"},{"location":"reference/declarai/python_parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> python_parser","text":""},{"location":"reference/declarai/python_parser/#declarai.python_parser","title":"python_parser","text":"<p>Modules:</p> Name Description <code>parser</code> <p>PythonParser</p>"},{"location":"reference/declarai/python_parser/magic_parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> magic_parser","text":""},{"location":"reference/declarai/python_parser/magic_parser/#declarai.python_parser.magic_parser","title":"magic_parser","text":""},{"location":"reference/declarai/python_parser/parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> parser","text":""},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser","title":"parser","text":"<p>PythonParser An interface to extract different parts of the provided python code into a simple metadata object.</p> <p>Classes:</p> Name Description <code>PythonParser</code> <p>A unified interface for accessing python parsed data.</p>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser","title":"PythonParser","text":"<pre><code>PythonParser(decorated: Any)\n</code></pre> <p>A unified interface for accessing python parsed data.</p> <p>Attributes:</p> Name Type Description <code>has_any_return_defs</code> <code>bool</code> <p>A return definition is any of the following:</p> <code>has_structured_return_type</code> <code>bool</code> <p>Except for the following types, a dedicated output parsing</p> Source code in <code>src/declarai/python_parser/parser.py</code> <pre><code>def __init__(self, decorated: Any):\n    self.is_func = inspect.isfunction(decorated)\n    self.is_class = inspect.isclass(decorated)\n    self.decorated = decorated\n\n    # Static attributes:\n    self.name = self.decorated.__name__\n\n    self._signature = inspect.signature(self.decorated)\n    self.signature_return_type = self.signature_return.type_\n\n    docstring = inspect.getdoc(self.decorated)\n    self._parsed_docstring = ReSTDocstringParser(docstring or \"\")\n    self.docstring_freeform = self._parsed_docstring.freeform\n    self.docstring_params = self._parsed_docstring.params\n    self.docstring_return = self._parsed_docstring.returns\n</code></pre>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser.has_any_return_defs","title":"has_any_return_defs  <code>cached</code> <code>property</code>","text":"<pre><code>has_any_return_defs: bool\n</code></pre> <p>A return definition is any of the following: - return type annotation - return reference in docstring - return referenced in magic placeholder  # TODO: Address magic reference as well.</p>"},{"location":"reference/declarai/python_parser/parser/#declarai.python_parser.parser.PythonParser.has_structured_return_type","title":"has_structured_return_type  <code>cached</code> <code>property</code>","text":"<pre><code>has_structured_return_type: bool\n</code></pre> <p>Except for the following types, a dedicated output parsing behavior is required to return the expected return type of the task.</p>"},{"location":"reference/declarai/python_parser/type_annotation_to_schema/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> type_annotation_to_schema","text":""},{"location":"reference/declarai/python_parser/type_annotation_to_schema/#declarai.python_parser.type_annotation_to_schema","title":"type_annotation_to_schema","text":"<p>Functions:</p> Name Description <code>type_annotation_to_str_schema</code> <p>This method accepts arbitrary types defined in the return annotation of a functions.</p>"},{"location":"reference/declarai/python_parser/type_annotation_to_schema/#declarai.python_parser.type_annotation_to_schema.type_annotation_to_str_schema","title":"type_annotation_to_str_schema","text":"<pre><code>type_annotation_to_str_schema(type_) -&gt; Optional[str]\n</code></pre> <p>This method accepts arbitrary types defined in the return annotation of a functions. Then creates a string representation of the annotation schema to be passed to the model.</p> Source code in <code>src/declarai/python_parser/type_annotation_to_schema.py</code> <pre><code>def type_annotation_to_str_schema(type_) -&gt; Optional[str]:\n\"\"\"\n    This method accepts arbitrary types defined in the return annotation of a functions.\n    Then creates a string representation of the annotation schema to be passed to the model.\n    \"\"\"\n    if type_.__module__ == \"builtins\":\n        if type_ in (str, int, float, bool):\n            return type_.__name__\n\n    if isinstance(type_, typing._GenericAlias):\n        root_name = type_._name\n        if not root_name:\n            if type_.__origin__ == typing.Union:\n                root_name = \"Union\"\n        properties = []\n        for sub_type in type_.__args__:\n            resolved_schema = resolve_to_json_schema(sub_type)\n            properties.append(resolve_pydantic_schema_recursive(resolved_schema))\n\n        if len(properties) &gt; 1:\n            resolved_str_schema = f\"{root_name}[{properties[0]}, {properties[1]}]\"\n        else:\n            resolved_str_schema = f\"{root_name}[{properties[0]}]\"\n        return schema_to_string_for_prompt(resolved_str_schema)\n\n    resolved_schema = resolve_to_json_schema(type_)\n    resolved_schema = resolve_pydantic_schema_recursive(resolved_schema)\n    str_schema = json.dumps(resolved_schema, indent=4)\n    return schema_to_string_for_prompt(str_schema)\n</code></pre>"},{"location":"reference/declarai/python_parser/types/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> types","text":""},{"location":"reference/declarai/python_parser/types/#declarai.python_parser.types","title":"types","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> docstring_parsers","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/#declarai.python_parser.docstring_parsers","title":"docstring_parsers","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/types/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> types","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types","title":"types","text":"<p>Classes:</p> Name Description <code>BaseDocStringParser</code> <p>Base class for docstring parsers.</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser","title":"BaseDocStringParser","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for docstring parsers.</p> <p>Attributes:</p> Name Type Description <code>freeform</code> <code>DocstringFreeform</code> <p>Return the freeform docstring</p> <code>params</code> <code>DocstringParams</code> <p>Return the params/arguments docstring</p> <code>returns</code> <code>DocstringReturn</code> <p>Return the return docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.freeform","title":"freeform  <code>property</code>","text":"<pre><code>freeform: DocstringFreeform\n</code></pre> <p>Return the freeform docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.params","title":"params  <code>property</code>","text":"<pre><code>params: DocstringParams\n</code></pre> <p>Return the params/arguments docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/types/#declarai.python_parser.docstring_parsers.types.BaseDocStringParser.returns","title":"returns  <code>property</code>","text":"<pre><code>returns: DocstringReturn\n</code></pre> <p>Return the return docstring</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/reST/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> reST","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/reST/#declarai.python_parser.docstring_parsers.reST","title":"reST","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> parser","text":""},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/#declarai.python_parser.docstring_parsers.reST.parser","title":"parser","text":"<p>Classes:</p> Name Description <code>ReSTDocstringParser</code> <p>As recommended by (PEP 287)[https://peps.python.org/pep-0287/],</p>"},{"location":"reference/declarai/python_parser/docstring_parsers/reST/parser/#declarai.python_parser.docstring_parsers.reST.parser.ReSTDocstringParser","title":"ReSTDocstringParser","text":"<pre><code>ReSTDocstringParser(docstring: str)\n</code></pre> <p>             Bases: <code>BaseDocStringParser</code></p> <p>As recommended by (PEP 287)[https://peps.python.org/pep-0287/], the recommended docstring format is the reStructuredText format (shortform - reST).</p> Source code in <code>src/declarai/python_parser/docstring_parsers/reST/parser.py</code> <pre><code>def __init__(self, docstring: str):\n    self.docstring = docstring\n</code></pre>"}]}